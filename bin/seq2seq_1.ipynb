{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"0vmDOXcru6CH"},"source":["# 1 - Seq2Seq with Neural Networks for Household Load Data"]},{"cell_type":"markdown","metadata":{"id":"anXQdBx1u6CL"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6109,"status":"ok","timestamp":1670944368236,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"9WoPuZYZu6CM"},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolaushouben\u001b[0m (\u001b[33mwattcast\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os, sys\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","import torch.optim as optim\n","import pytorch_lightning as pl\n","from pytorch_lightning import LightningModule\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from pytorch_lightning.loggers import WandbLogger\n","from PIL import Image\n","from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n","from io import BytesIO\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","#import plotly.express as px\n","\n","from scipy.stats import boxcox\n","\n","\n","import wandb\n","wandb.login()\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class ElectricalLoadDataset(Dataset):\n","    def __init__(self, data_dir, \n","                input_chunk_size,\n","                output_chunk_size, \n","                scaler = None,\n","                timeenc=False,\n","                split='train', \n","                train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, SFH = -1):\n","        \n","        \n","        self.input_chunk_size = input_chunk_size\n","        self.output_chunk_size = output_chunk_size\n","        self.scaler = scaler\n","        self.timeenc = timeenc\n","        self.SFH = SFH\n","\n","        load_series = self._load_data(data_dir)\n","\n","        if self.scaler is None and split == 'train':\n","            self.scaler = MinMaxScaler()\n","            self.scaler.fit(load_series[:int(train_ratio * len(load_series))])\n","\n","        if split == 'train':\n","            self.load_series = self.scaler.transform(load_series[:int(train_ratio * len(load_series))])\n","        elif split == 'val':\n","            self.load_series = self.scaler.transform(load_series[int(train_ratio * len(load_series)):int((train_ratio + val_ratio) * len(load_series))])\n","        elif split == 'test':\n","            self.load_series = self.scaler.transform(load_series[int((train_ratio + val_ratio) * len(load_series)):])\n","\n","\n","    def __len__(self):\n","        return len(self.load_series) - self.input_chunk_size - self.output_chunk_size + 1\n","\n","    def __getitem__(self, idx):\n","        input_chunk = self.load_series[idx:idx+self.input_chunk_size]\n","        output_chunk = self.load_series[idx+self.input_chunk_size-1:idx+self.input_chunk_size+self.output_chunk_size-1]\n","\n","\n","        return input_chunk, output_chunk\n","\n","    def _load_data(self, data_dir):\n","        df = pd.read_csv(os.path.join(data_dir, 'load_data_15min_watts.csv'), index_col=0, parse_dates=True)[:int(1e4)]\n","        df = df.iloc[:, [self.SFH]]\n","        if self.timeenc == 1:\n","            df = self.timeenc_1(df)\n","        else:\n","            pass\n","\n","        return df.values\n","            \n","    def timeenc_1(self, df):\n","        # minute of the day\n","        df['minute'] = df.index.hour * 60 + df.index.minute\n","        # day of the week\n","        df['dayofweek'] = df.index.dayofweek\n","        return df\n","\n","\n","    \n","class ElectricalLoadDataLoader(DataLoader):\n","    def __init__(self, *args, **kwargs):\n","        super(ElectricalLoadDataLoader, self).__init__(*args, **kwargs)\n","        self.collate_fn = self.collate_fn_\n","\n","    def collate_fn_(self, batch):\n","        input_chunks, output_chunks = zip(*batch)\n","        input_tensor = torch.FloatTensor(input_chunks)\n","        output_tensor = torch.FloatTensor(output_chunks)\n","        return input_tensor, output_tensor\n","    "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 96, 3]) torch.Size([16, 96, 3])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\nik\\AppData\\Local\\Temp\\ipykernel_12172\\3064791773.py:67: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n","  input_tensor = torch.FloatTensor(input_chunks)\n"]}],"source":["N_LAGS = 96\n","N_STEPS = 96\n","BATCH_SIZE = 16\n","HIDDEN_SIZE = 128\n","\n","\n","train = ElectricalLoadDataset(data_dir='../data', input_chunk_size=N_LAGS, output_chunk_size=N_STEPS, split='train', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, timeenc=1)\n","train_loader = ElectricalLoadDataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n","val = ElectricalLoadDataset(data_dir='../data', input_chunk_size=N_LAGS, output_chunk_size=N_STEPS, split='val', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, timeenc=1, scaler=train.scaler)\n","val_loader = ElectricalLoadDataLoader(val, batch_size=BATCH_SIZE, shuffle=False)\n","test = ElectricalLoadDataset(data_dir='../data', input_chunk_size=N_LAGS, output_chunk_size=N_STEPS, split='test', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, timeenc=1, scaler=train.scaler)\n","test_loader = ElectricalLoadDataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n","\n","\n","\n","for i , (input_chunk, output_chunk) in enumerate(train_loader):\n","    print(input_chunk.shape, output_chunk.shape)\n","    break\n"]},{"cell_type":"markdown","metadata":{},"source":["## The classic Seq2Seq model with a GRU encoder and decoder \n","\n","(Sutskever et al. 2014)\n","https://arxiv.org/abs/1409.3215"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class Encoder(LightningModule):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        \n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n","        out, hidden = self.gru(x, h0)\n","        return out, hidden\n","    \n","\n","class Decoder(LightningModule):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, input_size)\n","\n","\n","    def forward(self, x, hidden):\n","        out, hidden = self.gru(x, hidden)\n","        out = self.fc(out)\n","        return out, hidden\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class Seq2Seq(LightningModule):\n","    \n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        N, _, _ = src.shape\n","        N, output_chunk_size , output_size = trg.shape\n","        outputs = torch.zeros(N, output_chunk_size, output_size).to(self.device)\n","        out, hidden = self.encoder(src)\n","        inp = src[:,-1,:].unsqueeze(1)\n","        \n","        for t in range(0, output_chunk_size):\n","            output, hidden = self.decoder(inp, hidden)\n","            outputs[:,t,:] = output.squeeze(1)\n","            teacher_force = np.random.random() < teacher_forcing_ratio\n","            if self.training and teacher_force:\n","                inp = trg[:,t,:].unsqueeze(1)\n","            else:\n","                inp = output\n","        return outputs\n","\n","    def training_step(self, batch):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('val_loss', loss)\n","        if batch_idx == 0:\n","            buffers = self._plot_predictions(output, trg)\n","            # Combine the image buffers into a single image\n","            images = [np.array(Image.open(buffer)) for buffer in buffers]\n","            combined_image = np.concatenate(images, axis=1)\n","            # Log the combined image to WandB\n","            wandb.log({\"predictions_val_dataset\": wandb.Image(combined_image)})\n","        return loss\n","    \n","    def test_step(self, batch, batch_idx):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('test_loss', loss)\n","        if batch_idx == 0:\n","            buffers = self._plot_predictions(output, trg)\n","            # Combine the image buffers into a single image\n","            images = [np.array(Image.open(buffer)) for buffer in buffers]\n","            combined_image = np.concatenate(images, axis=1)\n","            # Log the combined image to WandB\n","            wandb.log({\"predictions_test_dataset\": wandb.Image(combined_image)})\n","        return loss\n","        \n","    def _plot_predictions(self, preds, actuals):\n","        preds = preds.detach().cpu().numpy()\n","        actuals = actuals.detach().cpu().numpy()\n","        buffers = []\n","        for i in range(preds.shape[0]):\n","            fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n","            # plotting the i-th sequence in the batch\n","            ax.plot(preds[i, :, 0], label='Predictions')\n","            ax.plot(actuals[i, :, 0], label='Actuals')\n","            ax.legend()\n","            # Convert the figure to an image buffer\n","            canvas = FigureCanvas(fig)\n","            buffer = BytesIO()\n","            canvas.print_figure(buffer, format='png')\n","            buffer.seek(0)\n","            # Close the figure to save memory\n","            plt.close(fig)\n","            # Append the image buffer to the list of buffers\n","            buffers.append(buffer)\n","        # Return the list of image buffers\n","        return buffers\n","\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=1e-3)\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:hgqpm1kt) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">logical-oath-22</strong> at: <a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/hgqpm1kt' target=\"_blank\">https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/hgqpm1kt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>.\\wandb\\run-20230403_114859-hgqpm1kt\\logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:hgqpm1kt). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"770566c480fc41699c1b6b2fec4aa977","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332417533, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.14.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>c:\\Users\\nik\\Desktop\\Berkeley_Projects\\LoadLanguageModel\\bin\\wandb\\run-20230403_115148-pc7jp4he</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/pc7jp4he' target=\"_blank\">atomic-wildflower-23</a></strong> to <a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting' target=\"_blank\">https://wandb.ai/wattcast/SFH%20Load%20Forecasting</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/pc7jp4he' target=\"_blank\">https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/pc7jp4he</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  rank_zero_warn(\n","c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n","  rank_zero_deprecation(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type    | Params\n","------------------------------------\n","0 | encoder | Encoder | 51.1 K\n","1 | decoder | Decoder | 51.5 K\n","------------------------------------\n","102 K     Trainable params\n","0         Non-trainable params\n","102 K     Total params\n","0.410     Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47c06a4a005d4f1185c5e9a1d35e3e44","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ab06cceffb64625b7023fb9ba198624","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ced95cd30bf041d0a78c31cd96dca1c7","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5575613ffca04a649303b530564f5712","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c12c9eb6f25a4865b734f56ec496e943","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1fdeb34399d49e58dd1023836be29ad","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f28f4363af214059ac79360837c8fb8d","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88ee061c34114ac3a74b72d01f5838fa","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n","  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"]}],"source":["run = wandb.init(project = 'SFH Load Forecasting')\n","wandb_logger = WandbLogger()\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","enc = Encoder(input_size=input_chunk.shape[-1], hidden_size=HIDDEN_SIZE, num_layers=1, dropout=0.2)\n","dec = Decoder(input_size=input_chunk.shape[-1], hidden_size=HIDDEN_SIZE, num_layers=1)\n","model = Seq2Seq(enc, dec)\n","\n","\n","cbs = [EarlyStopping(monitor='val_loss')]\n","trainer = pl.Trainer(max_epochs=20, logger = wandb_logger, gpus=1 if torch.cuda.is_available() else 0, callbacks=cbs)\n","trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e301fb5edfac4cfe9307696286428c0f","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","       Test metric             DataLoader 0\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","        test_loss           0.17294490337371826\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"]},{"data":{"text/plain":["[{'test_loss': 0.17294490337371826}]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["trainer.test(model, test_loader)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusion: \n","\n","We see that without teacher forcining the model is unable to learn from the data. \n","\n","Things to try next are:\n","\n","* Increase the number of epochs or the batch size during training to allow the model to learn more from the data.\n","\n","* Adjust the learning rate of the optimizer to help the model converge more quickly.\n","\n","* Experiment with different architectures or hyperparameters for the model, such as the number of layers or the dropout rate, to see if this improves performance.\n","\n","* Try using a different loss function, such as the mean absolute error (MAE) or DTW, to see if this helps the model better capture the patterns in the data.\n","\n","* Try one-shot, non recurrent approaches -> transformer"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9A_2CmwIA_Tf"},"source":["## Encoder-Decoder with Attention\n","\n","From the paper 'Learning to Align and Translate':\n","\n","https://arxiv.org/abs/1409.0473"]},{"cell_type":"code","execution_count":194,"metadata":{},"outputs":[],"source":["class EncoderBahdanau(LightningModule):\n","\n","    def __init__(self,input_size, hidden_size):\n","        super().__init__()  \n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first = True, bidirectional = True)\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size) # 2 for bidirection\n","\n","    def forward(self, input):    \n","        outputs, hidden = self.gru(input)  \n","        #outputs = [batch size, seq len, hid dim * num directions]\n","        #hidden = [batch size, num layers * num directions, hid dim]\n","        # the hiddens are stacked [forward_1, backward_1, forward_2, backward_2, ...]\n","        # we use the last two layers, and have to reshape it to be (batch_size, 2, hidden_size)\n","        hidden_con = hidden[-2:,:,:].permute(1, 0, 2).contiguous().view(input.shape[0], -1)\n","        hidden = torch.tanh(self.fc(hidden_con))\n","        return outputs, hidden\n","    \n","\n","class Attention(LightningModule):\n","    def __init__(self, hidden_size):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.attn = nn.Linear(self.hidden_size * 3, self.hidden_size) # 3 for hidden size of decoder, encoder, and attention\n","        self.v = nn.Linear(self.hidden_size, 1, bias = False) # 1 for one attention value per time step\n","\n","    def forward(self, hidden, encoder_outputs):\n","        '''The idea here is to use the hidden state of the decoder at each time step to calculate the attention weights for each time step of the encoder output.'''\n","        # hidden = [batch size, hidden_size] of the decoder, which at t0 is the last hidden state of the encoder\n","        # encoder_outputs = [batch size, src len, enc hid dim * 2] because bidirectional\n","        N, src_len, _ = encoder_outputs.shape\n","        # repeat decoder hidden state src_len times to calculate attention weights\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        # encoder_outputs = [batch size, src len, enc hid dim * 2]\n","        # hidden = [batch size, src len, hidden_size]\n","        energy_input = torch.cat((hidden, encoder_outputs), dim = 2)\n","        energy = torch.tanh(self.attn(energy_input)) # (N, src_len, hidden_size)\n","        # energy = [batch size, src len, hidden_size]\n","        # now the energy is the input to the v layer, which is a linear layer with a single output\n","        attention = self.v(energy).squeeze(2) # (N, src_len), this basically transforms the energy to a single value for each time step\n","        # attention= [batch size, src len]\n","        return F.softmax(attention, dim=1)\n","    \n","\n","class DecoderBahdanau(LightningModule):\n","    '''For the decoder, we will use the attention mechanism to calculate the context vector,\n","    which is a weighted sum of the encoder outputs, based on the attention weights. \n","    The context vector is then concatenated with the decoder input and passed through the GRU.'''\n","    def __init__(self, input_size, hidden_size):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(2*hidden_size + input_size, hidden_size, batch_first = True)\n","        self.fc_out = nn.Linear(hidden_size * 3 + input_size, 1)\n","        self.attention = Attention(hidden_size)\n","\n","    def forward(self, inp, hidden, encoder_outputs):\n","        # input = [N]\n","        # hidden = [N, hidden_size]\n","        # encoder_outputs = [N, src_len, hidden_size * 2]\n","\n","        a = self.attention(hidden, encoder_outputs) # (N, src_len)\n","        a = a.unsqueeze(1) # (N, 1, src_len)\n","\n","        weighted = torch.einsum('nis,nsk->nik', a, encoder_outputs) # (N, 1, hidden_size * 2)')\n","\n","        #inp = inp.unsqueeze(1) # (N, 1, input_size)\n","\n","        rnn_input = torch.cat((inp, weighted), dim = 2) # (1, N, hidden_size * 3)\n","\n","        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n","\n","        inp, output, weighted = inp.squeeze(1), output.squeeze(1), weighted.squeeze(1)\n","\n","        prediction_input = torch.cat((inp, output, weighted), dim = 1) # (N, hidden_size * 3 + input_size)\n","        \n","        prediction = self.fc_out(prediction_input) # (N, 1)\n","\n","        return prediction, hidden.squeeze(0)"]},{"cell_type":"markdown","metadata":{},"source":["### Seq2Seq Model Adjusted for Bahdanau Attention"]},{"cell_type":"code","execution_count":261,"metadata":{},"outputs":[],"source":["class Seq2SeqBahdenau(LightningModule):\n","    \n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        N, _, _ = src.shape\n","        N, output_chunk_size , _ = trg.shape\n","        outputs = torch.zeros(N, output_chunk_size).to(self.device)\n","        encoder_outputs, hidden = self.encoder(src)\n","        inp = src[:,-1,:].unsqueeze(1)\n","\n","        print(output_chunk_size)\n","        \n","        for t in range(0, output_chunk_size):\n","            output, hidden = self.decoder(inp, hidden, encoder_outputs)\n","            print(outputs.shape)\n","            print(output.shape)\n","            outputs[:,t] = output.squeeze(1)\n","            teacher_force = np.random.random() < teacher_forcing_ratio\n","            if self.training and teacher_force:\n","                inp = trg[:,t,:].unsqueeze(1)\n","            else:\n","                inp = output\n","        return outputs\n","\n","    def training_step(self, batch):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('val_loss', loss)\n","        if batch_idx == 0:\n","            buffers = self._plot_predictions(output, trg)\n","            # Combine the image buffers into a single image\n","            images = [np.array(Image.open(buffer)) for buffer in buffers]\n","            combined_image = np.concatenate(images, axis=1)\n","            # Log the combined image to WandB\n","            wandb.log({\"predictions_val_dataset\": wandb.Image(combined_image)})\n","        return loss\n","    \n","    def test_step(self, batch, batch_idx):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('test_loss', loss)\n","        if batch_idx == 0:\n","            buffers = self._plot_predictions(output, trg)\n","            # Combine the image buffers into a single image\n","            images = [np.array(Image.open(buffer)) for buffer in buffers]\n","            combined_image = np.concatenate(images, axis=1)\n","            # Log the combined image to WandB\n","            wandb.log({\"predictions_test_dataset\": wandb.Image(combined_image)})\n","        return loss\n","        \n","    def _plot_predictions(self, preds, actuals):\n","        preds = preds.detach().cpu().numpy()\n","        actuals = actuals.detach().cpu().numpy()\n","        buffers = []\n","        for i in range(preds.shape[0]):\n","            fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n","            # plotting the i-th sequence in the batch\n","            ax.plot(preds[i, :, 0], label='Predictions')\n","            ax.plot(actuals[i, :, 0], label='Actuals')\n","            ax.legend()\n","            # Convert the figure to an image buffer\n","            canvas = FigureCanvas(fig)\n","            buffer = BytesIO()\n","            canvas.print_figure(buffer, format='png')\n","            buffer.seek(0)\n","            # Close the figure to save memory\n","            plt.close(fig)\n","            # Append the image buffer to the list of buffers\n","            buffers.append(buffer)\n","        # Return the list of image buffers\n","        return buffers\n","\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":262,"metadata":{},"outputs":[],"source":["enc = EncoderBahdanau(3, 128)\n","\n","output, hidden = enc(input_chunk)"]},{"cell_type":"code","execution_count":263,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([16, 96])"]},"execution_count":263,"metadata":{},"output_type":"execute_result"}],"source":["att = Attention(128)\n","\n","attention = att(hidden, output)\n","\n","attention.shape"]},{"cell_type":"code","execution_count":264,"metadata":{},"outputs":[],"source":["\n","dec = DecoderBahdanau(3, 128)\n","prediction, hidden = dec(output_chunk[:,:1,:], hidden, output)"]},{"cell_type":"code","execution_count":265,"metadata":{},"outputs":[],"source":["model_test = Seq2SeqBahdenau(enc, dec)"]},{"cell_type":"code","execution_count":266,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["96\n","torch.Size([16, 96])\n","torch.Size([16, 1])\n","torch.Size([16, 96])\n","torch.Size([16, 1])\n","torch.Size([16, 96])\n","torch.Size([16, 1])\n","torch.Size([16, 96])\n","torch.Size([16, 1])\n"]},{"ename":"IndexError","evalue":"Dimension out of range (expected to be in range of [-2, 1], but got 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[266], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_test(input_chunk, output_chunk)\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[261], line 18\u001b[0m, in \u001b[0;36mSeq2SeqBahdenau.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(output_chunk_size)\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, output_chunk_size):\n\u001b[0;32m---> 18\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(inp, hidden, encoder_outputs)\n\u001b[1;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[194], line 68\u001b[0m, in \u001b[0;36mDecoderBahdanau.forward\u001b[0;34m(self, inp, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m weighted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mnis,nsk->nik\u001b[39m\u001b[39m'\u001b[39m, a, encoder_outputs) \u001b[39m# (N, 1, hidden_size * 2)')\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m#inp = inp.unsqueeze(1) # (N, 1, input_size)\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m rnn_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((inp, weighted), dim \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m) \u001b[39m# (1, N, hidden_size * 3)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(rnn_input, hidden\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     72\u001b[0m inp, output, weighted \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), weighted\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n","\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"]}],"source":["model_test(input_chunk, output_chunk)"]},{"cell_type":"markdown","metadata":{"id":"Eu7RyX0KLeJj"},"source":["### Trying out the model"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  rank_zero_warn(\n","/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n","  rank_zero_deprecation(\n","GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","\n","  | Name    | Type            | Params\n","--------------------------------------------\n","0 | encoder | EncoderBahdanau | 135 K \n","1 | decoder | DecoderBahdanau | 199 K \n","--------------------------------------------\n","334 K     Trainable params\n","0         Non-trainable params\n","334 K     Total params\n","1.337     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n","  rank_zero_warn(\n","/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([16, 256])\n"]},{"ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 4 and 3","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[174], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m cbs \u001b[39m=\u001b[39m [EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m      8\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, logger \u001b[39m=\u001b[39m wandb_logger, gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m, callbacks\u001b[39m=\u001b[39mcbs)\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model_bahdenau, train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader, val_dataloaders\u001b[39m=\u001b[39;49mval_loader)\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1204\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1203\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1206\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1276\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1276\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1280\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    138\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1496\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","Cell \u001b[0;32mIn[172], line 34\u001b[0m, in \u001b[0;36mSeq2SeqBahdenau.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     33\u001b[0m     src, trg \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 34\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(src, trg)\n\u001b[1;32m     35\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(output, trg)\n\u001b[1;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, loss)\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[172], line 16\u001b[0m, in \u001b[0;36mSeq2SeqBahdenau.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     13\u001b[0m inp \u001b[39m=\u001b[39m src[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, output_chunk_size):\n\u001b[0;32m---> 16\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(inp, hidden, encoder_outputs)\n\u001b[1;32m     17\u001b[0m     outputs[:,t,:] \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m     teacher_force \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m teacher_forcing_ratio\n","File \u001b[0;32m~/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[171], line 69\u001b[0m, in \u001b[0;36mDecoderBahdanau.forward\u001b[0;34m(self, inp, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m weighted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mnis,nsk->nik\u001b[39m\u001b[39m'\u001b[39m, a, encoder_outputs) \u001b[39m# (N, 1, hidden_size * 2)')\u001b[39;00m\n\u001b[1;32m     67\u001b[0m inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m# (N, 1, input_size)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m rnn_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((inp, weighted), dim \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m) \u001b[39m# (1, N, hidden_size * 3)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(rnn_input, hidden\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     73\u001b[0m inp, output, weighted \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), weighted\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 3"]}],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","enc = EncoderBahdanau(input_size=3, hidden_size=128)\n","dec = DecoderBahdanau(input_size=3, hidden_size=128)\n","model_bahdenau = Seq2SeqBahdenau(enc, dec)\n","wandb_logger = WandbLogger(project = 'SFH Load Forecasting', name = \"Bahdenau_Attention\")\n","\n","cbs = [EarlyStopping(monitor='val_loss')]\n","trainer = pl.Trainer(max_epochs=20, logger = wandb_logger, gpus=1 if torch.cuda.is_available() else 0, callbacks=cbs)\n","trainer.fit(model_bahdenau, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"]},{"cell_type":"markdown","metadata":{"id":"4aS9xB-MyRJb"},"source":["# Convolutional Seq2Seq\n","\n","https://arxiv.org/abs/1705.03122"]},{"cell_type":"markdown","metadata":{"id":"clVlGhAcWz2p"},"source":["### Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6vBiRNCGPoI"},"outputs":[],"source":["class EncoderConv(nn.Module):\n","  def __init__(self, input_size, emb_size, hid_size, n_layers, kernel_size, dropout, device, max_length= 100):\n","    super(EncoderConv, self).__init__()\n","\n","    assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n","\n","    self.device = device\n","    # The scale variable is used by the authors to \"ensure that the variance\n","    # throughout the network does not change dramatically\".\n","    # The performance of the model seems to vary wildly\n","    # using different seeds if this is not used.\n","    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device) # ~ 0.7\n","    self.tok_emb = nn.Linear(input_size,emb_size)\n","    self.pos_emb = nn.Embedding(max_length, emb_size)\n","    self.emb2hid = nn.Linear(emb_size, hid_size)\n","    self.hid2emb = nn.Linear(hid_size, emb_size)\n","    self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_size,\n","                                          out_channels = 2*hid_size,\n","                                          kernel_size = kernel_size,\n","                                          padding = (kernel_size -1) // 2\n","                                          )\n","                                for _ in range(n_layers)])\n","    self.dropout = nn.Dropout(dropout)\n","\n","\n","  def forward(self, src):\n","    batch_size = src.shape[0]\n","    src_len = src.shape[1]\n","\n","    #pos = [batch_size, src_len] (of the longest seq in the batch)\n","    pos = torch.arange(0,src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","    tok_embedded = self.tok_emb(src)\n","    pos_embedded = self.pos_emb(pos)\n","\n","    #element-wise summing of embeddings\n","    #embedded = [batch_size, src_len, emb_size]\n","    embedded = self.dropout(tok_embedded + pos_embedded)\n","\n","    #conv_input = [batch_size, src_len, hidden_size]\n","    conv_input = self.emb2hid(embedded)\n","\n","    conv_input = torch.einsum('ijk->ikj', conv_input)\n","\n","    # conv likes dims = [batch_size, hidden_size, src_len]; n = batches, C_in = channels / hidden, L = length\n","    # https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n","    for i, conv in enumerate(self.convs):\n","\n","      conved = conv(self.dropout(conv_input))\n","\n","      conved = F.glu(conved, dim = 1) # 1 (k) dim is now [hidden_size * 2]\n","\n","      #residual\n","\n","      #print(self.scale)\n","      conved = (conved + conv_input) * self.scale # \n","\n","      conv_input = conved\n","\n","\n","    conved = torch.einsum('ikj->ijk', conved)\n","    conved = self.hid2emb(conved)\n","\n","    combined = (conved + embedded) * self.scale\n","\n","    return conved, combined\n","\n","\n","\n","\n","                                                    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cc2eVF8qSQnY"},"outputs":[],"source":["enc_conv = EncoderConv(input_size=1, emb_size=1, hid_size=3, n_layers=2, kernel_size= 3, dropout = 0.5, device = device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669120724939,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"fCOP0M09SYmo","outputId":"a7743648-3e59-4b88-f420-a11eb163a08e"},"outputs":[{"data":{"text/plain":["torch.Size([8, 48, 1])"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["conved, combined = enc_conv(src)\n"]},{"cell_type":"markdown","metadata":{"id":"8XylnsnuWxcw"},"source":["### Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CloxXBmPSzAx"},"outputs":[],"source":["class DecoderConv(nn.Module):\n","\n","  def __init__(self,\n","               input_size,\n","               emb_size,\n","               hid_size,\n","               output_size,\n","               n_layers,\n","               kernel_size,\n","               dropout,\n","               trg_pad_idx,\n","               device, \n","               max_length=100):\n","    \n","    super(DecoderConv, self).__init__()\n","    \n","    self.device = device\n","    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device) # ~ 0.7\n","    self.input_size = input_size\n","    self.hid_size = hid_size\n","    self.kernel_size = kernel_size\n","    self.trg_pad_idx = trg_pad_idx\n","\n","    self.dropout = nn.Dropout(dropout)\n","    self.tok_emb = nn.Linear(input_size, emb_size)\n","    self.pos_emb = nn.Embedding(max_length, emb_size)\n","    self.emb2hid = nn.Linear(emb_size, hid_size)\n","    self.hid2emb = nn.Linear(hid_size, emb_size)\n","    self.att_hid2emb = nn.Linear(hid_size, emb_size)\n","    self.att_emb2hid = nn.Linear(emb_size, hid_size)\n","    self.fc_out = nn.Linear(emb_size, output_size)\n","\n","\n","    self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_size,\n","                                          out_channels = 2*hid_size,\n","                                          kernel_size = kernel_size\n","                                          )\n","                                for _ in range(n_layers)])\n","\n","\n","\n","  def calc_att(self, embedded, conved, encoder_conved, encoder_combined):\n","\n","    \n","    #encoder_conved = [N, src, emb]\n","    #encoder_combined = [N, src, emb]\n","\n","    #conved; it comes out [N, hid_size, trg] from the conv1d layer \n","    conved_permuted = torch.einsum('nht->nth', conved)\n","    # -> [N, trg, hid]\n","\n","\n","    conved_emb = self.att_hid2emb(conved_permuted)\n","    # [N, trg, emb]\n","\n","    combined = (conved_emb + embedded) * self.scale\n","\n","    encoder_conved_perm = torch.einsum('nse->nes', encoder_conved)\n","\n","    energy = torch.einsum('nte,nes->nts', [combined, encoder_conved_perm])\n","\n","    attention = F.softmax(energy, dim = 2) #over the source dimension, because later the encoders are weighted\n","\n","    attended_encoding = torch.einsum('nts, nse->nte', attention, encoder_combined)\n","\n","    attended_encoding = self.att_emb2hid(attended_encoding)\n","    #[N, trg, hid]\n","\n","    attended_combined = (attended_encoding.permute(0,2,1) + conved) * self.scale\n","\n","\n","    return attention, attended_combined\n","\n","\n","\n","  def forward(self, trg, encoder_conved, encoder_combined):\n","\n","    batch_size = trg.shape[0]\n","    trg_len = trg.shape[1]\n","\n","    pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","    tok_embedded = self.tok_emb(trg)\n","    pos_embedded = self.pos_emb(pos)\n","\n","    embedded = self.dropout(tok_embedded + pos_embedded)\n","\n","    conv_input = self.emb2hid(embedded)\n","\n","    conv_input = torch.einsum('nth->nht', conv_input)\n","\n","    for i, conv in enumerate(self.convs):\n","\n","      conv_input = self.dropout(conv_input)\n","\n","      # padding\n","      padding = torch.zeros(batch_size, self.hid_size, self.kernel_size -1).fill_(self.trg_pad_idx).to(self.device)\n","\n","      padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n","      # [N, h, t+k-1]\n","\n","      conved = conv(padded_conv_input)\n","      # [N, 2*h, t]\n","\n","      conved = F.glu(conved, dim = 1) #glu halves the dimension, this is why we setup the convs that way\n","\n","      # [N, h , t]\n","\n","      attention, conved = self.calc_att(embedded, conved, encoder_conved, encoder_combined)\n","      # att: [N, t, s]\n","      #conved: [N, h, t]\n","\n","      conved = (conved + conv_input) * self.scale\n","\n","      conv_input = conved #pass to the next layer\n","\n","\n","    conved = self.hid2emb(conved.permute(0,2,1))\n","    # [N, t, e]\n","\n","    output = self.fc_out(self.dropout(conved))\n","    # [N, t, 1]\n","\n","    return output, attention\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1669122496281,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"Ey48-WHo5Sgo","outputId":"c3a1f31b-c01e-48ac-e061-3c1636e54c64"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n"]}],"source":["\n","batch = iter(trainloader).next()\n","src, trg = batch\n","src = torch.tensor(src, dtype=torch.float32).to(device)\n","trg = torch.tensor(trg, dtype=torch.float32).to(device)\n","\n","\n","\n","dec = DecoderConv(\n","               input_size = 1,\n","               emb_size = 1,\n","               hid_size = 3,\n","               output_size = 1,\n","               n_layers= 2,\n","               kernel_size=3,\n","               dropout=0.1,\n","               trg_pad_idx= 1,\n","               device=device, \n","               max_length=100)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKKOln8YWgFP"},"outputs":[],"source":["output, attn = dec(trg, conved, combined)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDsmWD1EXVuU"},"outputs":[],"source":["class Seq2SeqConv(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \n","    def forward(self, src, trg):\n","        \n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n","           \n","        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n","        #encoder_conved is output from final encoder conv. block\n","        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n","        #  positional embeddings \n","        encoder_conved, encoder_combined = self.encoder(src)\n","            \n","        #encoder_conved = [batch size, src len, emb dim]\n","        #encoder_combined = [batch size, src len, emb dim]\n","        \n","        #calculate predictions of next words\n","        #output is a batch of predictions for each word in the trg sentence\n","        #attention a batch of attention scores across the src sentence for \n","        #  each word in the trg sentence\n","        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n","        \n","        #output = [batch size, trg len - 1, output dim]\n","        #attention = [batch size, trg len - 1, src len]\n","        \n","        return output, attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNY32j7XdLUK"},"outputs":[],"source":["TRG_PAD_IDX = 1e-10\n","\n","\n","enc_conv = EncoderConv(input_size=1,\n","                       emb_size=1,\n","                       hid_size=3,\n","                       n_layers=2, \n","                       kernel_size= 3,\n","                       dropout = 0.5, \n","                       device = device)\n","\n","dec_conv = DecoderConv(\n","               input_size = 1,\n","               emb_size = 1,\n","               hid_size = 3,\n","               output_size = 1,\n","               n_layers= 2,\n","               kernel_size=3,\n","               dropout=0.1,\n","               trg_pad_idx= TRG_PAD_IDX,\n","               device=device, \n","               max_length=100)\n","\n","model_conv = Seq2SeqConv(enc_conv, dec_conv).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":877,"status":"ok","timestamp":1669122747518,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"-LYE_6kvdapt","outputId":"db0212b4-4c01-4869-b383-f53496643fd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has 476 trainable parameters\n"]}],"source":["print(f'The model has {count_parameters(model_conv):,} trainable parameters')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FolC6hBcdsDC"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SE9Py5ecli_1"},"outputs":[],"source":["def generate_square_subsequent_mask(dim1: int, dim2: int):\n","    \"\"\"\n","    Generates an upper-triangular matrix of -inf, with zeros on diag.\n","    Modified from: \n","    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","    Args:\n","        dim1: int, for both src and tgt masking, this must be target sequence\n","              length\n","        dim2: int, for src masking this must be encoder sequence length (i.e. \n","              the length of the input sequence to the model), \n","              and for tgt masking, this must be target sequence length \n","    Return:\n","        A Tensor of shape [dim1, dim2]\n","    \"\"\"\n","    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1669124758583,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"UfssyVUoljwQ","outputId":"b1339f86-7e33-4d88-d0f8-9b6f7a3858f4"},"outputs":[{"data":{"text/plain":["tensor([[0., -inf, -inf],\n","        [0., 0., -inf],\n","        [0., 0., 0.]])"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["generate_square_subsequent_mask(3,3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPdoNCdglpjw"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["cylh-CMwV9wB","9A_2CmwIA_Tf"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"gpu2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"2d017bf32283ba7f67b14df218f7d5e330099bbfa9a17c2165be9519c3d88acb"}}},"nbformat":4,"nbformat_minor":0}
