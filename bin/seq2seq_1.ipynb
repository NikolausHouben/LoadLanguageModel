{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"0vmDOXcru6CH"},"source":["# 1 - Seq2Seq with Neural Networks for Household Load Data"]},{"cell_type":"markdown","metadata":{"id":"anXQdBx1u6CL"},"source":["## Imports"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":6109,"status":"ok","timestamp":1670944368236,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"9WoPuZYZu6CM"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"data":{"text/plain":["True"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["import os, sys\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","import torch.optim as optim\n","import pytorch_lightning as pl\n","from pytorch_lightning import LightningModule\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from pytorch_lightning.loggers import WandbLogger\n","from PIL import Image\n","from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n","from io import BytesIO\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","#import plotly.express as px\n","\n","from scipy.stats import boxcox\n","\n","\n","import wandb\n","wandb.login()\n","\n","\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["class ElectricalLoadDataset(Dataset):\n","    def __init__(self, data_dir, \n","                input_chunk_size,\n","                output_chunk_size, \n","                scaler = None,\n","                timeenc=False,\n","                split='train', \n","                train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, SFH = -1, boxcox_trafo=False):\n","        \n","        \n","        self.input_chunk_size = input_chunk_size\n","        self.output_chunk_size = output_chunk_size\n","        self.scaler = scaler\n","        self.timeenc = timeenc\n","        self.SFH = SFH\n","        self.boxcox_trafo = boxcox_trafo\n","\n","        load_series = self._load_data(data_dir)\n","\n","        if self.scaler is None and split == 'train':\n","            self.scaler = MinMaxScaler()\n","            self.scaler.fit(load_series[:int(train_ratio * len(load_series))])\n","\n","        if split == 'train':\n","            self.load_series = self.scaler.transform(load_series[:int(train_ratio * len(load_series))])\n","        elif split == 'val':\n","            self.load_series = self.scaler.transform(load_series[int(train_ratio * len(load_series)):int((train_ratio + val_ratio) * len(load_series))])\n","        elif split == 'test':\n","            self.load_series = self.scaler.transform(load_series[int((train_ratio + val_ratio) * len(load_series)):])\n","\n","\n","    def __len__(self):\n","        return len(self.load_series) - self.input_chunk_size - self.output_chunk_size + 1\n","\n","    def __getitem__(self, idx):\n","        input_chunk = self.load_series[idx:idx+self.input_chunk_size]\n","        output_chunk = self.load_series[idx+self.input_chunk_size:idx+self.input_chunk_size+self.output_chunk_size]\n","        if self.boxcox_trafo:\n","            input_chunk, output_chunk = self._boxcox(input_chunk, output_chunk)\n","\n","        return input_chunk, output_chunk\n","\n","    def _load_data(self, data_dir):\n","        df = pd.read_csv(os.path.join(data_dir, 'load_data_15min_watts.csv'), index_col=0, parse_dates=True)[:int(1e4)]\n","        df = df.iloc[:, [self.SFH]]\n","        if self.timeenc == 1:\n","            df = self.timeenc_1(df)\n","        elif self.timeenc == 2:\n","            df = self.timeenc_2(df)\n","\n","        return df.values\n","            \n","    def timeenc_1(self, df):\n","        # minute of the day\n","        df['minute'] = df.index.hour * 60 + df.index.minute\n","        # day of the week\n","        df['dayofweek'] = df.index.dayofweek\n","        return df\n","    \n","    def timeenc_2(self, df):\n","        #trigonometric encoding\n","        df['sin_time'] = np.sin(2*np.pi*(df.index.minute/60 + df.index.hour/24))\n","        df['cos_time'] = np.cos(2*np.pi*(df.index.minute/60 + df.index.hour/24))\n","        return df\n","\n","    #TODO: fix bug here\n","    def _boxcox(self, input_chunk, output_chunk):\n","        '''This function applies the boxcox transformation to the input and output chunks.'''\n","        trafo_inp = np.zeros_like(input_chunk)\n","        trafo_out = np.zeros_like(output_chunk)\n","        N, T, D = input_chunk.shape\n","        for i in range(N):\n","            # make data positive\n","            data_in = (input_chunk[i] + 1e-6).flatten()\n","            data_out = (output_chunk[i] + 1e-6).flatten()\n","            bx_in, *_  = boxcox(data_in)\n","            bx_out,*_  = boxcox(data_out)\n","            #make bx into shape (96,1)\n","            trafo_inp[i,:,:] = bx_in.reshape(T,D)\n","            trafo_out[i,:,:] = bx_out.reshape(T,D)\n","\n","        return trafo_inp, trafo_out\n","\n","    \n","class ElectricalLoadDataLoader(DataLoader):\n","    def __init__(self, *args, **kwargs):\n","        super(ElectricalLoadDataLoader, self).__init__(*args, **kwargs)\n","        self.collate_fn = self.collate_fn_\n","\n","    def collate_fn_(self, batch):\n","        input_chunks, output_chunks = zip(*batch)\n","        input_tensor = torch.FloatTensor(input_chunks)\n","        output_tensor = torch.FloatTensor(output_chunks)\n","        return input_tensor, output_tensor\n","    "]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["train = ElectricalLoadDataset(data_dir='../data', input_chunk_size=96, output_chunk_size=96, split='train', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, timeenc=1)\n","train_loader = ElectricalLoadDataLoader(train, batch_size=16, shuffle=True)\n","val = ElectricalLoadDataset(data_dir='../data', scaler = train.scaler, input_chunk_size=96, output_chunk_size=96, split='val', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,  timeenc=1)\n","val_loader = ElectricalLoadDataLoader(val, batch_size=16, shuffle=True)\n","test = ElectricalLoadDataset(data_dir='../data', scaler = train.scaler, input_chunk_size=96, output_chunk_size=96, split='test', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,  timeenc=1)\n","test_loader = ElectricalLoadDataLoader(test, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 96, 3]) torch.Size([16, 96, 3])\n"]}],"source":["for i , (input_chunk, output_chunk) in enumerate(train_loader):\n","    print(input_chunk.shape, output_chunk.shape)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["## The classic Seq2Seq model with a GRU encoder and decoder \n","\n","(Sutskever et al. 2014)\n","https://arxiv.org/abs/1409.3215"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["class Encoder(LightningModule):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        \n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n","        out, hidden = self.gru(x, h0)\n","        return out, hidden\n","    \n","\n","class Decoder(LightningModule):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, input_size)\n","\n","\n","    def forward(self, x, hidden):\n","        out, hidden = self.gru(x, hidden)\n","        out = self.fc(out)\n","        return out, hidden\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["class Seq2Seq(LightningModule):\n","    \n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        N, _, _ = src.shape\n","        N, output_chunk_size , output_size = trg.shape\n","        outputs = torch.zeros(N, output_chunk_size, output_size).to(self.device)\n","        out, hidden = self.encoder(src)\n","        inp = src[:,-1,:].unsqueeze(1)\n","        \n","        for t in range(0, output_chunk_size):\n","            output, hidden = self.decoder(inp, hidden)\n","            outputs[:,t,:] = output.squeeze(1)\n","            teacher_force = np.random.random() < teacher_forcing_ratio\n","            if self.training:\n","                inp = trg[:,t,:].unsqueeze(1) if teacher_force else output\n","            else:\n","                inp = output\n","        return outputs\n","\n","    def training_step(self, batch):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.l1_loss(output, trg)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('val_loss', loss)\n","        if batch_idx == 0:\n","            buffers = self._plot_predictions(output, trg)\n","            # Combine the image buffers into a single image\n","            images = [np.array(Image.open(buffer)) for buffer in buffers]\n","            combined_image = np.concatenate(images, axis=1)\n","            # Log the combined image to WandB\n","            wandb.log({\"predictions_val_dataset\": wandb.Image(combined_image)})\n","        return loss\n","    \n","    def test_step(self, batch, batch_idx):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('test_loss', loss)\n","        if batch_idx == 0:\n","            buffers = self._plot_predictions(output, trg)\n","            # Combine the image buffers into a single image\n","            images = [np.array(Image.open(buffer)) for buffer in buffers]\n","            combined_image = np.concatenate(images, axis=1)\n","            # Log the combined image to WandB\n","            wandb.log({\"predictions_test_dataset\": wandb.Image(combined_image)})\n","        return loss\n","        \n","    def _plot_predictions(self, preds, actuals):\n","        preds = preds.detach().cpu().numpy()\n","        actuals = actuals.detach().cpu().numpy()\n","        buffers = []\n","        for i in range(preds.shape[0]):\n","            fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n","            # plotting the i-th sequence in the batch\n","            ax.plot(preds[i, :, 0], label='Predictions')\n","            ax.plot(actuals[i, :, 0], label='Actuals')\n","            ax.legend()\n","            # Convert the figure to an image buffer\n","            canvas = FigureCanvas(fig)\n","            buffer = BytesIO()\n","            canvas.print_figure(buffer, format='png')\n","            buffer.seek(0)\n","            # Close the figure to save memory\n","            plt.close(fig)\n","            # Append the image buffer to the list of buffers\n","            buffers.append(buffer)\n","        # Return the list of image buffers\n","        return buffers\n","\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=1e-3)\n","    "]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:1) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▃▃▃▃▃▆▆▆▆▆█████▁▁▁▁▁▃▃▃▃▃▆▆▆▆▆█▁▁▁▁</td></tr><tr><td>train_loss</td><td>▂▁▁▁▂▂▁▂▂▁▂▁▂▁▁▂▁▂▁▁▆▆▆▅▅▆▆▆▆▆▆▆▅▆▆▆█▅▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▆▆▆▇▇████▁▂▂▂▃▃▄▄▄▅▅▆▆▆▇▁▂▂▂</td></tr><tr><td>val_loss</td><td>▁▆▄█▂▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_loss</td><td>0.03884</td></tr><tr><td>trainer/global_step</td><td>449</td></tr><tr><td>val_loss</td><td>0.01122</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">GRU_with_timeenc_no_teacher_forcing</strong> at: <a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/1' target=\"_blank\">https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/1</a><br/>Synced 6 W&B file(s), 12 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>.\\wandb\\run-20230330_114229-1\\logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:1). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59505c7e37fc4e439a09ef38cac04306","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.14.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>c:\\Users\\nik\\Desktop\\Berkeley_Projects\\LoadLanguageModel\\bin\\wandb\\run-20230330_120531-3</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/3' target=\"_blank\">GRU_timeenc_no_teacher_forcing</a></strong> to <a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting' target=\"_blank\">https://wandb.ai/wattcast/SFH%20Load%20Forecasting</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/3' target=\"_blank\">https://wandb.ai/wattcast/SFH%20Load%20Forecasting/runs/3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type    | Params\n","------------------------------------\n","0 | encoder | Encoder | 51.1 K\n","1 | decoder | Decoder | 51.5 K\n","------------------------------------\n","102 K     Trainable params\n","0         Non-trainable params\n","102 K     Total params\n","0.410     Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"277da4fc2c254de98f651c2541883472","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f72b13fcbf6404081d2936cd05ffffb","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56012f08286646638f7c3432bb68acfc","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcfabde1fecd4f6ca648c53970af08c2","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d662ba55d77545c5b7b9b57afd6fc779","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f3d5565f2d14d839280d93c87b99893","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","enc = Encoder(input_size=3, hidden_size=128, num_layers=1, dropout=0.2)\n","dec = Decoder(input_size=3, hidden_size=128, num_layers=1)\n","model = Seq2Seq(enc, dec)\n","\n","wandb_logger = WandbLogger(project = 'SFH Load Forecasting', name = \"GRU_timeenc_no_teacher_forcing\", id = \"3\")\n","\n","cbs = [EarlyStopping(monitor='val_loss')]\n","trainer = pl.Trainer(max_epochs=20, logger = wandb_logger, gpus=1 if torch.cuda.is_available() else 0, callbacks=cbs)\n","trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e301fb5edfac4cfe9307696286428c0f","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","       Test metric             DataLoader 0\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","        test_loss           0.17294490337371826\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"]},{"data":{"text/plain":["[{'test_loss': 0.17294490337371826}]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["trainer.test(model, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Conclusion: \n","\n","We see that without teacher forcining the model is unable to learn from the data. \n","\n","Things to try next are:\n","\n","* Increase the number of epochs or the batch size during training to allow the model to learn more from the data.\n","\n","* Adjust the learning rate of the optimizer to help the model converge more quickly.\n","\n","* Experiment with different architectures or hyperparameters for the model, such as the number of layers or the dropout rate, to see if this improves performance.\n","\n","* Try using a different loss function, such as the mean absolute error (MAE) or DTW, to see if this helps the model better capture the patterns in the data.\n","\n","* Try one-shot approaches"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9A_2CmwIA_Tf"},"source":["## Encoder-Decoder with Attention\n","\n","From the paper 'Learning to Align and Translate':\n","\n","https://arxiv.org/abs/1409.0473"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jb6qiiRO9HcZ"},"outputs":[],"source":["class EncoderBad(nn.Module):\n","\n","    def __init__(self,input_size, hidden_size):\n","        super(EncoderBad, self).__init__()  \n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first = True, bidirectional = True)\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","\n","    def forward(self, input):    \n","        outputs, hidden = self.gru(input)  \n","\n","        hidden_con = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n","        hidden = torch.tanh(self.fc(hidden_con))\n","\n","        print(hidden.shape)\n","        return outputs, hidden\n","\n","class Attention(nn.Module):\n","  def __init__(self, hidden_size):\n","    super(Attention, self).__init__()\n","\n","    self.attn = nn.Linear(hidden_size * 3, hidden_size)\n","    self.v = nn.Linear(hidden_size, 1, bias = False)\n","\n","  def forward(self, hidden, encoder_outputs):\n","\n","\n","    batch_size = encoder_outputs.shape[0]\n","    src_len = encoder_outputs.shape[1]\n","\n","    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1) #we open up the hidden tensor at index 1, and repeat it src length time at this index\n","\n","    energy_input = torch.cat((hidden, encoder_outputs), dim = 2)\n","\n","    energy = torch.tanh(self.attn(energy_input))\n","    attention = self.v(energy)\n","\n","    return F.softmax(attention, dim = 1).squeeze(-1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1668677706244,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"ZvaqxQNe_DzS","outputId":"9c1d7af2-2978-4bf6-b2b1-167384e3b23c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n"]}],"source":["src, trg = iter(trainloader).next()\n","\n","src = torch.tensor(src, dtype=torch.float32).to(device)\n","trg = torch.tensor(trg, dtype=torch.float32).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1668677706521,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"epUCX-cjEEjC","outputId":"9d31d901-9be7-468f-8fe7-49dc6e0fb269"},"outputs":[{"data":{"text/plain":["torch.Size([8, 48, 1])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["src.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1668677706522,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"75WBmCt6_QHn","outputId":"2618a2e4-e13f-4a31-ebf7-d442b45446e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 48, 6]) torch.Size([2, 8, 3])\n","torch.Size([8, 3])\n"]}],"source":["encoder = EncoderBad(1,3)\n","\n","outputs, hidden = encoder(src)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1668605788605,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"8nkjvht0EBQ0","outputId":"5e37a9ad-deae-450b-c8bb-9ac822d20e7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 3]) torch.Size([8, 48, 6])\n"]}],"source":["print(hidden.shape, outputs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcmbJh12FiIV"},"outputs":[],"source":["attn = Attention(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eu7RyX0KLeJj"},"outputs":[],"source":["class DecoderBad(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, attention):\n","      super(DecoderBad, self).__init__()   \n","      \n","      self.output_size = output_size\n","      self.attention = attention\n","      \n","      self.hidden_size = hidden_size\n","      self.gru = nn.GRU(input_size=hidden_size * 2 + input_size,  hidden_size=hidden_size, batch_first = True)\n","      self.fc_out = nn.Linear(hidden_size*3 + input_size, output_size)\n","  \n","    def forward(self, input, hidden, encoder_outputs):\n","\n","\n","      # encoder_outputs = [batch_size, src_len, hidden_size * 2]\n","      # hidden = [batch_size, hidden_size]\n","      # input = [batch_size] only one word / number per timestep\n","\n","      input = input.unsqueeze(-1)\n","      \n","      # input = [batch_size, 1]\n","\n","      # a = [batch_size, src_len]\n","      a = self.attention(hidden, encoder_outputs)\n","      a = a.unsqueeze(1) \n","      # a = [batch_size,1, src_len]\n","\n","      weighted = torch.einsum('bki, bjh-> bkh',[a,encoder_outputs])\n","      #weighted = [batch_size, 1, 2*hidden_size]\n","\n","      rnn_input = torch.cat((input, weighted), dim = 2)\n","\n","\n","      #rnn_input = [batch_size, 1, hidden_size * 2 + input_size]\n","\n","      output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n","\n","\n","      prediction = self.fc_out(torch.cat((output, weighted, input), dim = 2)).squeeze(-1)\n","\n","\n","      return prediction, hidden.squeeze(0)\n","      \n","\n","class Seq2Seq_Bad(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super(Seq2Seq_Bad, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        \n","    def forward(self, src, trg, teacher_force_ratio = 0.5):\n","        batch_size = src.shape[0]\n","        target_len = trg.shape[1]\n","\n","        outputs = torch.zeros(target_len, batch_size, self.decoder.output_size).to(self.device)\n","        \n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        x = src[:,-1,:] #this is the first input to the decoder (the most previous known lag)\n","\n","        for t in range(1, target_len):\n","                \n","            output, hidden = self.decoder(x, hidden, encoder_outputs)\n","\n","            best_guess = output\n","\n","            ground_truth = trg[:,t,:]\n","            \n","            x = ground_truth if random.random() < teacher_force_ratio else best_guess #sometimes we take the real value, sometimes we take the predicted value\n","\n","            outputs[t] = torch.einsum('ij-> ji', output).unsqueeze(-1)\n","\n","        return outputs\n","\n","\n","    def predict(self, src, n_ahead):\n","\n","        batch_size = src.shape[0]\n","        outputs = torch.zeros(n_ahead, batch_size, 1).to(self.device)\n","        \n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        x = src[:,-1,:] #this is the first input to the decoder (the most previous known lag)\n","\n","        for t in range(1, n_ahead):\n","            \n","            output, hidden = self.decoder(x, hidden, encoder_outputs)\n","\n","            outputs[t] = torch.einsum('ij-> ji', output).unsqueeze(-1)\n","\n","        return outputs[1:]\n"]},{"cell_type":"markdown","metadata":{"id":"4aS9xB-MyRJb"},"source":["# Convolutional Seq2Seq\n","\n","https://arxiv.org/abs/1705.03122"]},{"cell_type":"markdown","metadata":{"id":"clVlGhAcWz2p"},"source":["### Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6vBiRNCGPoI"},"outputs":[],"source":["class EncoderConv(nn.Module):\n","  def __init__(self, input_size, emb_size, hid_size, n_layers, kernel_size, dropout, device, max_length= 100):\n","    super(EncoderConv, self).__init__()\n","\n","    assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n","\n","    self.device = device\n","    # The scale variable is used by the authors to \"ensure that the variance\n","    # throughout the network does not change dramatically\".\n","    # The performance of the model seems to vary wildly\n","    # using different seeds if this is not used.\n","    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device) # ~ 0.7\n","    self.tok_emb = nn.Linear(input_size,emb_size)\n","    self.pos_emb = nn.Embedding(max_length, emb_size)\n","    self.emb2hid = nn.Linear(emb_size, hid_size)\n","    self.hid2emb = nn.Linear(hid_size, emb_size)\n","    self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_size,\n","                                          out_channels = 2*hid_size,\n","                                          kernel_size = kernel_size,\n","                                          padding = (kernel_size -1) // 2\n","                                          )\n","                                for _ in range(n_layers)])\n","    self.dropout = nn.Dropout(dropout)\n","\n","\n","  def forward(self, src):\n","    batch_size = src.shape[0]\n","    src_len = src.shape[1]\n","\n","    #pos = [batch_size, src_len] (of the longest seq in the batch)\n","    pos = torch.arange(0,src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","    tok_embedded = self.tok_emb(src)\n","    pos_embedded = self.pos_emb(pos)\n","\n","    #element-wise summing of embeddings\n","    #embedded = [batch_size, src_len, emb_size]\n","    embedded = self.dropout(tok_embedded + pos_embedded)\n","\n","    #conv_input = [batch_size, src_len, hidden_size]\n","    conv_input = self.emb2hid(embedded)\n","\n","    conv_input = torch.einsum('ijk->ikj', conv_input)\n","\n","    # conv likes dims = [batch_size, hidden_size, src_len]; n = batches, C_in = channels / hidden, L = length\n","    # https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n","    for i, conv in enumerate(self.convs):\n","\n","      conved = conv(self.dropout(conv_input))\n","\n","      conved = F.glu(conved, dim = 1) # 1 (k) dim is now [hidden_size * 2]\n","\n","      #residual\n","\n","      #print(self.scale)\n","      conved = (conved + conv_input) * self.scale # \n","\n","      conv_input = conved\n","\n","\n","    conved = torch.einsum('ikj->ijk', conved)\n","    conved = self.hid2emb(conved)\n","\n","    combined = (conved + embedded) * self.scale\n","\n","    return conved, combined\n","\n","\n","\n","\n","                                                    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cc2eVF8qSQnY"},"outputs":[],"source":["enc_conv = EncoderConv(input_size=1, emb_size=1, hid_size=3, n_layers=2, kernel_size= 3, dropout = 0.5, device = device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669120724939,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"fCOP0M09SYmo","outputId":"a7743648-3e59-4b88-f420-a11eb163a08e"},"outputs":[{"data":{"text/plain":["torch.Size([8, 48, 1])"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["conved, combined = enc_conv(src)\n"]},{"cell_type":"markdown","metadata":{"id":"8XylnsnuWxcw"},"source":["### Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CloxXBmPSzAx"},"outputs":[],"source":["class DecoderConv(nn.Module):\n","\n","  def __init__(self,\n","               input_size,\n","               emb_size,\n","               hid_size,\n","               output_size,\n","               n_layers,\n","               kernel_size,\n","               dropout,\n","               trg_pad_idx,\n","               device, \n","               max_length=100):\n","    \n","    super(DecoderConv, self).__init__()\n","    \n","    self.device = device\n","    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device) # ~ 0.7\n","    self.input_size = input_size\n","    self.hid_size = hid_size\n","    self.kernel_size = kernel_size\n","    self.trg_pad_idx = trg_pad_idx\n","\n","    self.dropout = nn.Dropout(dropout)\n","    self.tok_emb = nn.Linear(input_size, emb_size)\n","    self.pos_emb = nn.Embedding(max_length, emb_size)\n","    self.emb2hid = nn.Linear(emb_size, hid_size)\n","    self.hid2emb = nn.Linear(hid_size, emb_size)\n","    self.att_hid2emb = nn.Linear(hid_size, emb_size)\n","    self.att_emb2hid = nn.Linear(emb_size, hid_size)\n","    self.fc_out = nn.Linear(emb_size, output_size)\n","\n","\n","    self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_size,\n","                                          out_channels = 2*hid_size,\n","                                          kernel_size = kernel_size\n","                                          )\n","                                for _ in range(n_layers)])\n","\n","\n","\n","  def calc_att(self, embedded, conved, encoder_conved, encoder_combined):\n","\n","    \n","    #encoder_conved = [N, src, emb]\n","    #encoder_combined = [N, src, emb]\n","\n","    #conved; it comes out [N, hid_size, trg] from the conv1d layer \n","    conved_permuted = torch.einsum('nht->nth', conved)\n","    # -> [N, trg, hid]\n","\n","\n","    conved_emb = self.att_hid2emb(conved_permuted)\n","    # [N, trg, emb]\n","\n","    combined = (conved_emb + embedded) * self.scale\n","\n","    encoder_conved_perm = torch.einsum('nse->nes', encoder_conved)\n","\n","    energy = torch.einsum('nte,nes->nts', [combined, encoder_conved_perm])\n","\n","    attention = F.softmax(energy, dim = 2) #over the source dimension, because later the encoders are weighted\n","\n","    attended_encoding = torch.einsum('nts, nse->nte', attention, encoder_combined)\n","\n","    attended_encoding = self.att_emb2hid(attended_encoding)\n","    #[N, trg, hid]\n","\n","    attended_combined = (attended_encoding.permute(0,2,1) + conved) * self.scale\n","\n","\n","    return attention, attended_combined\n","\n","\n","\n","  def forward(self, trg, encoder_conved, encoder_combined):\n","\n","    batch_size = trg.shape[0]\n","    trg_len = trg.shape[1]\n","\n","    pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","    tok_embedded = self.tok_emb(trg)\n","    pos_embedded = self.pos_emb(pos)\n","\n","    embedded = self.dropout(tok_embedded + pos_embedded)\n","\n","    conv_input = self.emb2hid(embedded)\n","\n","    conv_input = torch.einsum('nth->nht', conv_input)\n","\n","    for i, conv in enumerate(self.convs):\n","\n","      conv_input = self.dropout(conv_input)\n","\n","      # padding\n","      padding = torch.zeros(batch_size, self.hid_size, self.kernel_size -1).fill_(self.trg_pad_idx).to(self.device)\n","\n","      padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n","      # [N, h, t+k-1]\n","\n","      conved = conv(padded_conv_input)\n","      # [N, 2*h, t]\n","\n","      conved = F.glu(conved, dim = 1) #glu halves the dimension, this is why we setup the convs that way\n","\n","      # [N, h , t]\n","\n","      attention, conved = self.calc_att(embedded, conved, encoder_conved, encoder_combined)\n","      # att: [N, t, s]\n","      #conved: [N, h, t]\n","\n","      conved = (conved + conv_input) * self.scale\n","\n","      conv_input = conved #pass to the next layer\n","\n","\n","    conved = self.hid2emb(conved.permute(0,2,1))\n","    # [N, t, e]\n","\n","    output = self.fc_out(self.dropout(conved))\n","    # [N, t, 1]\n","\n","    return output, attention\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1669122496281,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"Ey48-WHo5Sgo","outputId":"c3a1f31b-c01e-48ac-e061-3c1636e54c64"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n"]}],"source":["\n","batch = iter(trainloader).next()\n","src, trg = batch\n","src = torch.tensor(src, dtype=torch.float32).to(device)\n","trg = torch.tensor(trg, dtype=torch.float32).to(device)\n","\n","\n","\n","dec = DecoderConv(\n","               input_size = 1,\n","               emb_size = 1,\n","               hid_size = 3,\n","               output_size = 1,\n","               n_layers= 2,\n","               kernel_size=3,\n","               dropout=0.1,\n","               trg_pad_idx= 1,\n","               device=device, \n","               max_length=100)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKKOln8YWgFP"},"outputs":[],"source":["output, attn = dec(trg, conved, combined)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDsmWD1EXVuU"},"outputs":[],"source":["class Seq2SeqConv(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \n","    def forward(self, src, trg):\n","        \n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n","           \n","        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n","        #encoder_conved is output from final encoder conv. block\n","        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n","        #  positional embeddings \n","        encoder_conved, encoder_combined = self.encoder(src)\n","            \n","        #encoder_conved = [batch size, src len, emb dim]\n","        #encoder_combined = [batch size, src len, emb dim]\n","        \n","        #calculate predictions of next words\n","        #output is a batch of predictions for each word in the trg sentence\n","        #attention a batch of attention scores across the src sentence for \n","        #  each word in the trg sentence\n","        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n","        \n","        #output = [batch size, trg len - 1, output dim]\n","        #attention = [batch size, trg len - 1, src len]\n","        \n","        return output, attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNY32j7XdLUK"},"outputs":[],"source":["TRG_PAD_IDX = 1e-10\n","\n","\n","enc_conv = EncoderConv(input_size=1,\n","                       emb_size=1,\n","                       hid_size=3,\n","                       n_layers=2, \n","                       kernel_size= 3,\n","                       dropout = 0.5, \n","                       device = device)\n","\n","dec_conv = DecoderConv(\n","               input_size = 1,\n","               emb_size = 1,\n","               hid_size = 3,\n","               output_size = 1,\n","               n_layers= 2,\n","               kernel_size=3,\n","               dropout=0.1,\n","               trg_pad_idx= TRG_PAD_IDX,\n","               device=device, \n","               max_length=100)\n","\n","model_conv = Seq2SeqConv(enc_conv, dec_conv).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":877,"status":"ok","timestamp":1669122747518,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"-LYE_6kvdapt","outputId":"db0212b4-4c01-4869-b383-f53496643fd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has 476 trainable parameters\n"]}],"source":["print(f'The model has {count_parameters(model_conv):,} trainable parameters')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FolC6hBcdsDC"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SE9Py5ecli_1"},"outputs":[],"source":["def generate_square_subsequent_mask(dim1: int, dim2: int):\n","    \"\"\"\n","    Generates an upper-triangular matrix of -inf, with zeros on diag.\n","    Modified from: \n","    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","    Args:\n","        dim1: int, for both src and tgt masking, this must be target sequence\n","              length\n","        dim2: int, for src masking this must be encoder sequence length (i.e. \n","              the length of the input sequence to the model), \n","              and for tgt masking, this must be target sequence length \n","    Return:\n","        A Tensor of shape [dim1, dim2]\n","    \"\"\"\n","    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1669124758583,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"UfssyVUoljwQ","outputId":"b1339f86-7e33-4d88-d0f8-9b6f7a3858f4"},"outputs":[{"data":{"text/plain":["tensor([[0., -inf, -inf],\n","        [0., 0., -inf],\n","        [0., 0., 0.]])"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["generate_square_subsequent_mask(3,3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPdoNCdglpjw"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["cylh-CMwV9wB","9A_2CmwIA_Tf"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 ('anot_trafo')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"3c18b8131a0c0dcb919ebaf33b65a863d5d3544c2a9ff0f6f29debb8767984d8"}}},"nbformat":4,"nbformat_minor":0}
