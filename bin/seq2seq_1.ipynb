{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"0vmDOXcru6CH"},"source":["# 1 - Seq2Seq with Neural Networks for Household Load Data"]},{"cell_type":"markdown","metadata":{"id":"anXQdBx1u6CL"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6109,"status":"ok","timestamp":1670944368236,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"9WoPuZYZu6CM"},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolaushouben\u001b[0m (\u001b[33mwattcast\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os, sys\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","import torch.optim as optim\n","import pytorch_lightning as pl\n","from pytorch_lightning import LightningModule\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from pytorch_lightning.loggers import WandbLogger\n","from PIL import Image\n","from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n","from io import BytesIO\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","#import plotly.express as px\n","\n","from scipy.stats import boxcox\n","\n","\n","import wandb\n","wandb.login()\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class ElectricalLoadDataset(Dataset):\n","    def __init__(self, data_dir, \n","                input_chunk_size,\n","                output_chunk_size, \n","                scaler = None,\n","                timeenc=False,\n","                split='train', \n","                train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, SFH = -1, boxcox_trafo=False):\n","        \n","        \n","        self.input_chunk_size = input_chunk_size\n","        self.output_chunk_size = output_chunk_size\n","        self.scaler = scaler\n","        self.timeenc = timeenc\n","        self.SFH = SFH\n","        self.boxcox_trafo = boxcox_trafo\n","\n","        load_series = self._load_data(data_dir)\n","\n","        if self.scaler is None and split == 'train':\n","            self.scaler = MinMaxScaler()\n","            self.scaler.fit(load_series[:int(train_ratio * len(load_series))])\n","\n","        if split == 'train':\n","            self.load_series = self.scaler.transform(load_series[:int(train_ratio * len(load_series))])\n","        elif split == 'val':\n","            self.load_series = self.scaler.transform(load_series[int(train_ratio * len(load_series)):int((train_ratio + val_ratio) * len(load_series))])\n","        elif split == 'test':\n","            self.load_series = self.scaler.transform(load_series[int((train_ratio + val_ratio) * len(load_series)):])\n","\n","\n","    def __len__(self):\n","        return len(self.load_series) - self.input_chunk_size - self.output_chunk_size + 1\n","\n","    def __getitem__(self, idx):\n","        input_chunk = self.load_series[idx:idx+self.input_chunk_size]\n","        output_chunk = self.load_series[idx+self.input_chunk_size:idx+self.input_chunk_size+self.output_chunk_size]\n","        if self.boxcox_trafo:\n","            input_chunk = self.boxy(input_chunk)\n","            output_chunk = self.boxy(output_chunk)\n","\n","\n","        return input_chunk, output_chunk\n","\n","    def _load_data(self, data_dir):\n","        df = pd.read_csv(os.path.join(data_dir, 'load_data_15min_watts.csv'), index_col=0, parse_dates=True)[:int(1e4)]\n","        df = df.iloc[:, [self.SFH]]\n","        if self.timeenc == 1:\n","            df = self.timeenc_1(df)\n","        elif self.timeenc == 2:\n","            df = self.timeenc_2(df)\n","\n","        return df.values\n","            \n","    def timeenc_1(self, df):\n","        # minute of the day\n","        df['minute'] = df.index.hour * 60 + df.index.minute\n","        # day of the week\n","        df['dayofweek'] = df.index.dayofweek\n","        return df\n","    \n","    def timeenc_2(self, df):\n","        #trigonometric encoding\n","        df['sin_time'] = np.sin(2*np.pi*(df.index.minute/60 + df.index.hour/24))\n","        df['cos_time'] = np.cos(2*np.pi*(df.index.minute/60 + df.index.hour/24))\n","        return df\n","    \n","    #TODO: fix bug here\n","    def boxy(self, sample):\n","        '''This function applies the boxcox transformation to the input and output chunks.'''\n","        data_transformed = np.zeros_like(sample)\n","        block_size, input_len = sample.shape\n","        # make data positive\n","        # apply boxcox transformation\n","        transformed, _ = boxcox(sample[:,:1].reshape(-1) + 1e-6)\n","        scaler = MinMaxScaler()\n","        transformed = scaler.fit_transform(transformed.reshape(-1,1))\n","        \n","        #make bx into shape (96,1)\n","        data_transformed[:,:1] = transformed.reshape(block_size,1)\n","        return data_transformed\n","\n","\n","    \n","class ElectricalLoadDataLoader(DataLoader):\n","    def __init__(self, *args, **kwargs):\n","        super(ElectricalLoadDataLoader, self).__init__(*args, **kwargs)\n","        self.collate_fn = self.collate_fn_\n","\n","    def collate_fn_(self, batch):\n","        input_chunks, output_chunks = zip(*batch)\n","        input_tensor = torch.FloatTensor(input_chunks)\n","        output_tensor = torch.FloatTensor(output_chunks)\n","        return input_tensor, output_tensor\n","    "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train = ElectricalLoadDataset(data_dir='../data', input_chunk_size=96, output_chunk_size=96, split='train', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, timeenc=1, boxcox_trafo=True)\n","train_loader = ElectricalLoadDataLoader(train, batch_size=16, shuffle=True)\n","val = ElectricalLoadDataset(data_dir='../data', scaler = train.scaler, input_chunk_size=96, output_chunk_size=96, split='val', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,  timeenc=1,boxcox_trafo=True)\n","val_loader = ElectricalLoadDataLoader(val, batch_size=16, shuffle=True)\n","test = ElectricalLoadDataset(data_dir='../data', scaler = train.scaler, input_chunk_size=96, output_chunk_size=96, split='test', train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,  timeenc=1,boxcox_trafo=True)\n","test_loader = ElectricalLoadDataLoader(test, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 96, 3]) torch.Size([16, 96, 3])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\nik\\AppData\\Local\\Temp\\ipykernel_6828\\1240816308.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n","  input_tensor = torch.FloatTensor(input_chunks)\n"]}],"source":["for i , (input_chunk, output_chunk) in enumerate(train_loader):\n","    print(input_chunk.shape, output_chunk.shape)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## The classic Seq2Seq model with a GRU encoder and decoder \n","\n","(Sutskever et al. 2014)\n","https://arxiv.org/abs/1409.3215"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class Encoder(LightningModule):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        \n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n","        out, hidden = self.gru(x, h0)\n","        return out, hidden\n","    \n","\n","class Decoder(LightningModule):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, input_size)\n","\n","\n","    def forward(self, x, hidden):\n","        out, hidden = self.gru(x, hidden)\n","        out = self.fc(out)\n","        return out, hidden\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class Seq2Seq(LightningModule):\n","    \n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        N, _, _ = src.shape\n","        N, output_chunk_size , output_size = trg.shape\n","        outputs = torch.zeros(N, output_chunk_size, output_size).to(self.device)\n","        out, hidden = self.encoder(src)\n","        inp = src[:,-1,:].unsqueeze(1)\n","        \n","        for t in range(0, output_chunk_size):\n","            output, hidden = self.decoder(inp, hidden)\n","            outputs[:,t,:] = output.squeeze(1)\n","            teacher_force = np.random.random() < teacher_forcing_ratio\n","            if self.training:\n","                inp = trg[:,t,:].unsqueeze(1) if teacher_force else output\n","            else:\n","                inp = output\n","        return outputs\n","\n","    def training_step(self, batch):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('val_loss', loss)\n","        if batch_idx == 0:\n","            buffers = self._plot_predictions(output, trg)\n","            # Combine the image buffers into a single image\n","            images = [np.array(Image.open(buffer)) for buffer in buffers]\n","            combined_image = np.concatenate(images, axis=1)\n","            # Log the combined image to WandB\n","            wandb.log({\"predictions_val_dataset\": wandb.Image(combined_image)})\n","        return loss\n","    \n","    def test_step(self, batch, batch_idx):\n","        src, trg = batch\n","        output = self(src, trg)\n","        loss = F.mse_loss(output, trg)\n","        self.log('test_loss', loss)\n","        if batch_idx == 0:\n","            buffers = self._plot_predictions(output, trg)\n","            # Combine the image buffers into a single image\n","            images = [np.array(Image.open(buffer)) for buffer in buffers]\n","            combined_image = np.concatenate(images, axis=1)\n","            # Log the combined image to WandB\n","            wandb.log({\"predictions_test_dataset\": wandb.Image(combined_image)})\n","        return loss\n","        \n","    def _plot_predictions(self, preds, actuals):\n","        preds = preds.detach().cpu().numpy()\n","        actuals = actuals.detach().cpu().numpy()\n","        buffers = []\n","        for i in range(preds.shape[0]):\n","            fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n","            # plotting the i-th sequence in the batch\n","            ax.plot(preds[i, :, 0], label='Predictions')\n","            ax.plot(actuals[i, :, 0], label='Actuals')\n","            ax.legend()\n","            # Convert the figure to an image buffer\n","            canvas = FigureCanvas(fig)\n","            buffer = BytesIO()\n","            canvas.print_figure(buffer, format='png')\n","            buffer.seek(0)\n","            # Close the figure to save memory\n","            plt.close(fig)\n","            # Append the image buffer to the list of buffers\n","            buffers.append(buffer)\n","        # Return the list of image buffers\n","        return buffers\n","\n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=1e-3)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","enc = Encoder(input_size=3, hidden_size=128, num_layers=1, dropout=0.2)\n","dec = Decoder(input_size=3, hidden_size=128, num_layers=1)\n","model = Seq2Seq(enc, dec)\n","wandb_logger = WandbLogger(project = 'SFH Load Forecasting', name = \"GRU_timeenc_box_1\", id = \"3\")\n","\n","cbs = [EarlyStopping(monitor='val_loss')]\n","trainer = pl.Trainer(max_epochs=20, logger = wandb_logger, gpus=1 if torch.cuda.is_available() else 0, callbacks=cbs)\n","trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n","wandb.finish()\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e301fb5edfac4cfe9307696286428c0f","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","       Test metric             DataLoader 0\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n","        test_loss           0.17294490337371826\n","────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"]},{"data":{"text/plain":["[{'test_loss': 0.17294490337371826}]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["trainer.test(model, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Conclusion: \n","\n","We see that without teacher forcining the model is unable to learn from the data. \n","\n","Things to try next are:\n","\n","* Increase the number of epochs or the batch size during training to allow the model to learn more from the data.\n","\n","* Adjust the learning rate of the optimizer to help the model converge more quickly.\n","\n","* Experiment with different architectures or hyperparameters for the model, such as the number of layers or the dropout rate, to see if this improves performance.\n","\n","* Try using a different loss function, such as the mean absolute error (MAE) or DTW, to see if this helps the model better capture the patterns in the data.\n","\n","* Try one-shot approaches"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9A_2CmwIA_Tf"},"source":["## Encoder-Decoder with Attention\n","\n","From the paper 'Learning to Align and Translate':\n","\n","https://arxiv.org/abs/1409.0473"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["class EncoderBahdanau(LightningModule):\n","\n","    def __init__(self,input_size, hidden_size):\n","        super().__init__()  \n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first = True, bidirectional = True)\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","\n","    def forward(self, input):    \n","        outputs, hidden = self.gru(input)  \n","        # the hiddens are stacked [forward_1, backward_1, forward_2, backward_2, ...]\n","        hidden_con = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n","        hidden = torch.tanh(self.fc(hidden_con))\n","        return outputs, hidden\n","    \n","\n","class Attention(LightningModule):\n","    def __init__(self, hidden_size):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.attn = nn.Linear(self.hidden_size * 3, self.hidden_size) # 3 for hidden size of decoder, encoder, and attention\n","        self.v = nn.Linear(self.hidden_size, 1, bias = False) # 1 for one attention value per time step\n","\n","    def forward(self, hidden, encoder_outputs):\n","        '''The idea here is to use the hidden state of the decoder at each time step to calculate the attention weights for each time step of the encoder output.'''\n","        # hidden = [batch size, hidden_size] of the decoder, which at t0 is the last hidden state of the encoder\n","        # encoder_outputs = [batch size, src len, enc hid dim * 2] because bidirectional\n","        N, src_len, _ = encoder_outputs.shape\n","        # repeat decoder hidden state src_len times to calculate attention weights\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        # encoder_outputs = [batch size, src len, enc hid dim * 2]\n","        # hidden = [batch size, src len, hidden_size]\n","        energy_input = torch.cat((hidden, encoder_outputs), dim = 2)\n","        energy = torch.tanh(self.attn(energy_input)) # (N, src_len, hidden_size)\n","        # energy = [batch size, src len, hidden_size]\n","        # now the energy is the input to the v layer, which is a linear layer with a single output\n","        attention = self.v(energy).squeeze(2) # (N, src_len),\n","        # attention= [batch size, src len]\n","        return F.softmax(attention, dim=1)\n","    \n","\n","class DecoderBahdanau(LightningModule):\n","    def __init__(self, output_size, hidden_size, dropout):\n","        super().__init__()\n","        self.output_size = output_size\n","        self.hidden_size = hidden_size\n","        self.dropout = dropout\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first = True)\n","        self.fc_out = nn.Linear(hidden_size * 2, output_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.attention = Attention(hidden_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["enc = EncoderBahdanau(3, 128)\n","\n","output, hidden = enc(input_chunk)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Sizes of tensors must match except in dimension 2. Expected size 96 but got size 1 for tensor number 1 in the list.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m attention \u001b[39m=\u001b[39m att(hidden, output)\n\u001b[0;32m      5\u001b[0m dec \u001b[39m=\u001b[39m DecoderBahdanau(\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m0.1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m output, hidden, attention \u001b[39m=\u001b[39m dec(output_chunk, hidden, output)\n","File \u001b[1;32mc:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[28], line 67\u001b[0m, in \u001b[0;36mDecoderBahdanau.forward\u001b[1;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     64\u001b[0m weighted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mnsl, nbs -> nbl\u001b[39m\u001b[39m'\u001b[39m, encoder_outputs, attention_weights)\n\u001b[0;32m     65\u001b[0m \u001b[39m# weighted = [N, 1, hidden_size * 2]\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# now we can concatenate the weighted sum with the input\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m rnn_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((\u001b[39minput\u001b[39;49m, weighted), dim \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m)\n\u001b[0;32m     68\u001b[0m \u001b[39m# rnn_input = [N, 1, input_size + hidden_size * 2]\u001b[39;00m\n\u001b[0;32m     69\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(rnn_input, hidden)\n","\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 96 but got size 1 for tensor number 1 in the list."]}],"source":["att = Attention(128)\n","\n","attention = att(hidden, output)\n","\n","dec = DecoderBahdanau(1, 128, 0.1)\n","\n","output, hidden, attention = dec(output_chunk, hidden, output)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"Jb6qiiRO9HcZ"},"outputs":[],"source":["class EncoderBad(nn.Module):\n","\n","    def __init__(self,input_size, hidden_size):\n","        super(EncoderBad, self).__init__()  \n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first = True, bidirectional = True)\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","\n","    def forward(self, input):    \n","        outputs, hidden = self.gru(input)  \n","\n","        hidden_con = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n","        hidden = torch.tanh(self.fc(hidden_con))\n","\n","        print(hidden.shape)\n","        return outputs, hidden\n","\n","class Attention(nn.Module):\n","  def __init__(self, hidden_size):\n","    super(Attention, self).__init__()\n","\n","    self.attn = nn.Linear(hidden_size * 3, hidden_size)\n","    self.v = nn.Linear(hidden_size, 1, bias = False)\n","\n","  def forward(self, hidden, encoder_outputs):\n","\n","    batch_size = encoder_outputs.shape[0]\n","    src_len = encoder_outputs.shape[1]\n","\n","    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1) #we open up the hidden tensor at index 1, and repeat it src length time at this index\n","\n","    energy_input = torch.cat((hidden, encoder_outputs), dim = 2)\n","\n","    energy = torch.tanh(self.attn(energy_input))\n","    attention = self.v(energy)\n","\n","    return F.softmax(attention, dim = 1).squeeze(-1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eu7RyX0KLeJj"},"outputs":[],"source":["class DecoderBad(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, attention):\n","      super(DecoderBad, self).__init__()   \n","      \n","      self.output_size = output_size\n","      self.attention = attention\n","      \n","      self.hidden_size = hidden_size\n","      self.gru = nn.GRU(input_size=hidden_size * 2 + input_size,  hidden_size=hidden_size, batch_first = True)\n","      self.fc_out = nn.Linear(hidden_size*3 + input_size, output_size)\n","  \n","    def forward(self, input, hidden, encoder_outputs):\n","\n","\n","      # encoder_outputs = [batch_size, src_len, hidden_size * 2]\n","      # hidden = [batch_size, hidden_size]\n","      # input = [batch_size] only one word / number per timestep\n","\n","      input = input.unsqueeze(-1)\n","      \n","      # input = [batch_size, 1]\n","\n","      # a = [batch_size, src_len]\n","      a = self.attention(hidden, encoder_outputs)\n","      a = a.unsqueeze(1) \n","      # a = [batch_size,1, src_len]\n","\n","      weighted = torch.einsum('bki, bjh-> bkh',[a,encoder_outputs])\n","      #weighted = [batch_size, 1, 2*hidden_size]\n","\n","      rnn_input = torch.cat((input, weighted), dim = 2)\n","\n","\n","      #rnn_input = [batch_size, 1, hidden_size * 2 + input_size]\n","\n","      output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n","\n","\n","      prediction = self.fc_out(torch.cat((output, weighted, input), dim = 2)).squeeze(-1)\n","\n","\n","      return prediction, hidden.squeeze(0)\n"]},{"cell_type":"markdown","metadata":{"id":"4aS9xB-MyRJb"},"source":["# Convolutional Seq2Seq\n","\n","https://arxiv.org/abs/1705.03122"]},{"cell_type":"markdown","metadata":{"id":"clVlGhAcWz2p"},"source":["### Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6vBiRNCGPoI"},"outputs":[],"source":["class EncoderConv(nn.Module):\n","  def __init__(self, input_size, emb_size, hid_size, n_layers, kernel_size, dropout, device, max_length= 100):\n","    super(EncoderConv, self).__init__()\n","\n","    assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n","\n","    self.device = device\n","    # The scale variable is used by the authors to \"ensure that the variance\n","    # throughout the network does not change dramatically\".\n","    # The performance of the model seems to vary wildly\n","    # using different seeds if this is not used.\n","    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device) # ~ 0.7\n","    self.tok_emb = nn.Linear(input_size,emb_size)\n","    self.pos_emb = nn.Embedding(max_length, emb_size)\n","    self.emb2hid = nn.Linear(emb_size, hid_size)\n","    self.hid2emb = nn.Linear(hid_size, emb_size)\n","    self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_size,\n","                                          out_channels = 2*hid_size,\n","                                          kernel_size = kernel_size,\n","                                          padding = (kernel_size -1) // 2\n","                                          )\n","                                for _ in range(n_layers)])\n","    self.dropout = nn.Dropout(dropout)\n","\n","\n","  def forward(self, src):\n","    batch_size = src.shape[0]\n","    src_len = src.shape[1]\n","\n","    #pos = [batch_size, src_len] (of the longest seq in the batch)\n","    pos = torch.arange(0,src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","    tok_embedded = self.tok_emb(src)\n","    pos_embedded = self.pos_emb(pos)\n","\n","    #element-wise summing of embeddings\n","    #embedded = [batch_size, src_len, emb_size]\n","    embedded = self.dropout(tok_embedded + pos_embedded)\n","\n","    #conv_input = [batch_size, src_len, hidden_size]\n","    conv_input = self.emb2hid(embedded)\n","\n","    conv_input = torch.einsum('ijk->ikj', conv_input)\n","\n","    # conv likes dims = [batch_size, hidden_size, src_len]; n = batches, C_in = channels / hidden, L = length\n","    # https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n","    for i, conv in enumerate(self.convs):\n","\n","      conved = conv(self.dropout(conv_input))\n","\n","      conved = F.glu(conved, dim = 1) # 1 (k) dim is now [hidden_size * 2]\n","\n","      #residual\n","\n","      #print(self.scale)\n","      conved = (conved + conv_input) * self.scale # \n","\n","      conv_input = conved\n","\n","\n","    conved = torch.einsum('ikj->ijk', conved)\n","    conved = self.hid2emb(conved)\n","\n","    combined = (conved + embedded) * self.scale\n","\n","    return conved, combined\n","\n","\n","\n","\n","                                                    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cc2eVF8qSQnY"},"outputs":[],"source":["enc_conv = EncoderConv(input_size=1, emb_size=1, hid_size=3, n_layers=2, kernel_size= 3, dropout = 0.5, device = device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669120724939,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"fCOP0M09SYmo","outputId":"a7743648-3e59-4b88-f420-a11eb163a08e"},"outputs":[{"data":{"text/plain":["torch.Size([8, 48, 1])"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["conved, combined = enc_conv(src)\n"]},{"cell_type":"markdown","metadata":{"id":"8XylnsnuWxcw"},"source":["### Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CloxXBmPSzAx"},"outputs":[],"source":["class DecoderConv(nn.Module):\n","\n","  def __init__(self,\n","               input_size,\n","               emb_size,\n","               hid_size,\n","               output_size,\n","               n_layers,\n","               kernel_size,\n","               dropout,\n","               trg_pad_idx,\n","               device, \n","               max_length=100):\n","    \n","    super(DecoderConv, self).__init__()\n","    \n","    self.device = device\n","    self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device) # ~ 0.7\n","    self.input_size = input_size\n","    self.hid_size = hid_size\n","    self.kernel_size = kernel_size\n","    self.trg_pad_idx = trg_pad_idx\n","\n","    self.dropout = nn.Dropout(dropout)\n","    self.tok_emb = nn.Linear(input_size, emb_size)\n","    self.pos_emb = nn.Embedding(max_length, emb_size)\n","    self.emb2hid = nn.Linear(emb_size, hid_size)\n","    self.hid2emb = nn.Linear(hid_size, emb_size)\n","    self.att_hid2emb = nn.Linear(hid_size, emb_size)\n","    self.att_emb2hid = nn.Linear(emb_size, hid_size)\n","    self.fc_out = nn.Linear(emb_size, output_size)\n","\n","\n","    self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_size,\n","                                          out_channels = 2*hid_size,\n","                                          kernel_size = kernel_size\n","                                          )\n","                                for _ in range(n_layers)])\n","\n","\n","\n","  def calc_att(self, embedded, conved, encoder_conved, encoder_combined):\n","\n","    \n","    #encoder_conved = [N, src, emb]\n","    #encoder_combined = [N, src, emb]\n","\n","    #conved; it comes out [N, hid_size, trg] from the conv1d layer \n","    conved_permuted = torch.einsum('nht->nth', conved)\n","    # -> [N, trg, hid]\n","\n","\n","    conved_emb = self.att_hid2emb(conved_permuted)\n","    # [N, trg, emb]\n","\n","    combined = (conved_emb + embedded) * self.scale\n","\n","    encoder_conved_perm = torch.einsum('nse->nes', encoder_conved)\n","\n","    energy = torch.einsum('nte,nes->nts', [combined, encoder_conved_perm])\n","\n","    attention = F.softmax(energy, dim = 2) #over the source dimension, because later the encoders are weighted\n","\n","    attended_encoding = torch.einsum('nts, nse->nte', attention, encoder_combined)\n","\n","    attended_encoding = self.att_emb2hid(attended_encoding)\n","    #[N, trg, hid]\n","\n","    attended_combined = (attended_encoding.permute(0,2,1) + conved) * self.scale\n","\n","\n","    return attention, attended_combined\n","\n","\n","\n","  def forward(self, trg, encoder_conved, encoder_combined):\n","\n","    batch_size = trg.shape[0]\n","    trg_len = trg.shape[1]\n","\n","    pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","    tok_embedded = self.tok_emb(trg)\n","    pos_embedded = self.pos_emb(pos)\n","\n","    embedded = self.dropout(tok_embedded + pos_embedded)\n","\n","    conv_input = self.emb2hid(embedded)\n","\n","    conv_input = torch.einsum('nth->nht', conv_input)\n","\n","    for i, conv in enumerate(self.convs):\n","\n","      conv_input = self.dropout(conv_input)\n","\n","      # padding\n","      padding = torch.zeros(batch_size, self.hid_size, self.kernel_size -1).fill_(self.trg_pad_idx).to(self.device)\n","\n","      padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n","      # [N, h, t+k-1]\n","\n","      conved = conv(padded_conv_input)\n","      # [N, 2*h, t]\n","\n","      conved = F.glu(conved, dim = 1) #glu halves the dimension, this is why we setup the convs that way\n","\n","      # [N, h , t]\n","\n","      attention, conved = self.calc_att(embedded, conved, encoder_conved, encoder_combined)\n","      # att: [N, t, s]\n","      #conved: [N, h, t]\n","\n","      conved = (conved + conv_input) * self.scale\n","\n","      conv_input = conved #pass to the next layer\n","\n","\n","    conved = self.hid2emb(conved.permute(0,2,1))\n","    # [N, t, e]\n","\n","    output = self.fc_out(self.dropout(conved))\n","    # [N, t, 1]\n","\n","    return output, attention\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1669122496281,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"Ey48-WHo5Sgo","outputId":"c3a1f31b-c01e-48ac-e061-3c1636e54c64"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n"]}],"source":["\n","batch = iter(trainloader).next()\n","src, trg = batch\n","src = torch.tensor(src, dtype=torch.float32).to(device)\n","trg = torch.tensor(trg, dtype=torch.float32).to(device)\n","\n","\n","\n","dec = DecoderConv(\n","               input_size = 1,\n","               emb_size = 1,\n","               hid_size = 3,\n","               output_size = 1,\n","               n_layers= 2,\n","               kernel_size=3,\n","               dropout=0.1,\n","               trg_pad_idx= 1,\n","               device=device, \n","               max_length=100)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKKOln8YWgFP"},"outputs":[],"source":["output, attn = dec(trg, conved, combined)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDsmWD1EXVuU"},"outputs":[],"source":["class Seq2SeqConv(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \n","    def forward(self, src, trg):\n","        \n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n","           \n","        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n","        #encoder_conved is output from final encoder conv. block\n","        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n","        #  positional embeddings \n","        encoder_conved, encoder_combined = self.encoder(src)\n","            \n","        #encoder_conved = [batch size, src len, emb dim]\n","        #encoder_combined = [batch size, src len, emb dim]\n","        \n","        #calculate predictions of next words\n","        #output is a batch of predictions for each word in the trg sentence\n","        #attention a batch of attention scores across the src sentence for \n","        #  each word in the trg sentence\n","        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n","        \n","        #output = [batch size, trg len - 1, output dim]\n","        #attention = [batch size, trg len - 1, src len]\n","        \n","        return output, attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNY32j7XdLUK"},"outputs":[],"source":["TRG_PAD_IDX = 1e-10\n","\n","\n","enc_conv = EncoderConv(input_size=1,\n","                       emb_size=1,\n","                       hid_size=3,\n","                       n_layers=2, \n","                       kernel_size= 3,\n","                       dropout = 0.5, \n","                       device = device)\n","\n","dec_conv = DecoderConv(\n","               input_size = 1,\n","               emb_size = 1,\n","               hid_size = 3,\n","               output_size = 1,\n","               n_layers= 2,\n","               kernel_size=3,\n","               dropout=0.1,\n","               trg_pad_idx= TRG_PAD_IDX,\n","               device=device, \n","               max_length=100)\n","\n","model_conv = Seq2SeqConv(enc_conv, dec_conv).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":877,"status":"ok","timestamp":1669122747518,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"-LYE_6kvdapt","outputId":"db0212b4-4c01-4869-b383-f53496643fd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has 476 trainable parameters\n"]}],"source":["print(f'The model has {count_parameters(model_conv):,} trainable parameters')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FolC6hBcdsDC"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SE9Py5ecli_1"},"outputs":[],"source":["def generate_square_subsequent_mask(dim1: int, dim2: int):\n","    \"\"\"\n","    Generates an upper-triangular matrix of -inf, with zeros on diag.\n","    Modified from: \n","    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","    Args:\n","        dim1: int, for both src and tgt masking, this must be target sequence\n","              length\n","        dim2: int, for src masking this must be encoder sequence length (i.e. \n","              the length of the input sequence to the model), \n","              and for tgt masking, this must be target sequence length \n","    Return:\n","        A Tensor of shape [dim1, dim2]\n","    \"\"\"\n","    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1669124758583,"user":{"displayName":"TimeCast AI","userId":"07364941519718465481"},"user_tz":-60},"id":"UfssyVUoljwQ","outputId":"b1339f86-7e33-4d88-d0f8-9b6f7a3858f4"},"outputs":[{"data":{"text/plain":["tensor([[0., -inf, -inf],\n","        [0., 0., -inf],\n","        [0., 0., 0.]])"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["generate_square_subsequent_mask(3,3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPdoNCdglpjw"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["cylh-CMwV9wB","9A_2CmwIA_Tf"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"gpu2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"2d017bf32283ba7f67b14df218f7d5e330099bbfa9a17c2165be9519c3d88acb"}}},"nbformat":4,"nbformat_minor":0}
