/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name    | Type    | Params
------------------------------------
0 | encoder | Encoder | 12.9 K
1 | decoder | Decoder | 12.9 K
------------------------------------
25.8 K    Trainable params
0         Non-trainable params
25.8 K    Total params
0.103     Total estimated model params size (MB)
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.
  rank_zero_warn(
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name    | Type    | Params
------------------------------------
0 | encoder | Encoder | 12.9 K
1 | decoder | Decoder | 12.9 K
------------------------------------
25.8 K    Trainable params
0         Non-trainable params
25.8 K    Total params
0.103     Total estimated model params size (MB)
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name    | Type    | Params
------------------------------------
0 | encoder | Encoder | 12.9 K
1 | decoder | Decoder | 12.9 K
------------------------------------
25.8 K    Trainable params
0         Non-trainable params
25.8 K    Total params
0.103     Total estimated model params size (MB)
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.
  rank_zero_warn(
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name    | Type    | Params
------------------------------------
0 | encoder | Encoder | 12.9 K
1 | decoder | Decoder | 12.9 K
------------------------------------
25.8 K    Trainable params
0         Non-trainable params
25.8 K    Total params
0.103     Total estimated model params size (MB)
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.
  rank_zero_warn(
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name    | Type    | Params
------------------------------------
0 | encoder | Encoder | 12.9 K
1 | decoder | Decoder | 12.9 K
------------------------------------
25.8 K    Trainable params
0         Non-trainable params
25.8 K    Total params
0.103     Total estimated model params size (MB)
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name    | Type    | Params
------------------------------------
0 | encoder | Encoder | 12.9 K
1 | decoder | Decoder | 12.9 K
------------------------------------
25.8 K    Trainable params
0         Non-trainable params
25.8 K    Total params
0.103     Total estimated model params size (MB)
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.
  rank_zero_warn(
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]cpu
cpu
Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  2.88it/s]cpu
cpu
Epoch 0:   0%|          | 0/2060 [00:00<?, ?it/s] cpu
cpu
Epoch 0:   0%|          | 1/2060 [00:00<03:37,  9.47it/s, loss=0.0105, v_num=ih91]cpu
cpu
Epoch 0:   0%|          | 2/2060 [00:00<03:06, 11.05it/s, loss=0.00954, v_num=ih91]cpu
cpu
Epoch 0:   0%|          | 3/2060 [00:00<02:55, 11.72it/s, loss=0.0121, v_num=ih91] cpu
cpu
Epoch 0:   0%|          | 4/2060 [00:00<02:58, 11.51it/s, loss=0.0124, v_num=ih91]cpu
cpu
Epoch 0:   0%|          | 5/2060 [00:00<02:52, 11.89it/s, loss=0.0108, v_num=ih91]cpu
cpu
Epoch 0:   0%|          | 6/2060 [00:00<02:49, 12.12it/s, loss=0.0103, v_num=ih91]cpu
cpu
Epoch 0:   0%|          | 7/2060 [00:00<02:46, 12.32it/s, loss=0.0103, v_num=ih91]cpu
cpu
Epoch 0:   0%|          | 8/2060 [00:00<02:45, 12.39it/s, loss=0.0099, v_num=ih91]cpu
cpu
Epoch 0:   0%|          | 9/2060 [00:00<02:42, 12.58it/s, loss=0.0106, v_num=ih91]cpu
cpu
Epoch 0:   0%|          | 10/2060 [00:00<02:41, 12.69it/s, loss=0.0105, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 11/2060 [00:00<02:41, 12.65it/s, loss=0.0108, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 12/2060 [00:00<02:42, 12.59it/s, loss=0.0109, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 13/2060 [00:01<02:43, 12.49it/s, loss=0.0113, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 14/2060 [00:01<02:44, 12.41it/s, loss=0.0108, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 15/2060 [00:01<02:44, 12.46it/s, loss=0.0109, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 16/2060 [00:01<02:43, 12.49it/s, loss=0.0111, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 17/2060 [00:01<02:43, 12.50it/s, loss=0.0117, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 18/2060 [00:01<02:44, 12.43it/s, loss=0.0112, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 19/2060 [00:01<02:43, 12.45it/s, loss=0.0112, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 20/2060 [00:01<02:43, 12.46it/s, loss=0.0111, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 21/2060 [00:01<02:43, 12.49it/s, loss=0.0111, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 22/2060 [00:01<02:43, 12.47it/s, loss=0.0114, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 23/2060 [00:01<02:43, 12.48it/s, loss=0.0112, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 24/2060 [00:01<02:42, 12.51it/s, loss=0.0114, v_num=ih91]cpu
cpu
Epoch 0:   1%|          | 25/2060 [00:01<02:42, 12.53it/s, loss=0.012, v_num=ih91] cpu
cpu
Epoch 0:   1%|▏         | 26/2060 [00:02<02:41, 12.56it/s, loss=0.0122, v_num=ih91]cpu
cpu
Epoch 0:   1%|▏         | 27/2060 [00:02<02:43, 12.45it/s, loss=0.0121, v_num=ih91]cpu
cpu
Epoch 0:   1%|▏         | 28/2060 [00:02<02:44, 12.38it/s, loss=0.0121, v_num=ih91]cpu
cpu
Epoch 0:   1%|▏         | 29/2060 [00:02<02:43, 12.41it/s, loss=0.0118, v_num=ih91]cpu
cpu
Epoch 0:   1%|▏         | 30/2060 [00:02<02:43, 12.44it/s, loss=0.0121, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 31/2060 [00:02<02:43, 12.43it/s, loss=0.0118, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 32/2060 [00:02<02:43, 12.43it/s, loss=0.0118, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 33/2060 [00:02<02:42, 12.44it/s, loss=0.0116, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 34/2060 [00:02<02:42, 12.46it/s, loss=0.0117, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 35/2060 [00:02<02:42, 12.49it/s, loss=0.0119, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 36/2060 [00:02<02:41, 12.53it/s, loss=0.0118, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 37/2060 [00:02<02:41, 12.56it/s, loss=0.0111, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 38/2060 [00:03<02:40, 12.59it/s, loss=0.0115, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 39/2060 [00:03<02:40, 12.59it/s, loss=0.0112, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 40/2060 [00:03<02:40, 12.58it/s, loss=0.0112, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 41/2060 [00:03<02:41, 12.54it/s, loss=0.011, v_num=ih91] cpu
cpu
Epoch 0:   2%|▏         | 42/2060 [00:03<02:41, 12.52it/s, loss=0.0106, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 43/2060 [00:03<02:41, 12.52it/s, loss=0.0106, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 44/2060 [00:03<02:40, 12.54it/s, loss=0.0105, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 45/2060 [00:03<02:40, 12.55it/s, loss=0.0101, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 46/2060 [00:03<02:40, 12.56it/s, loss=0.0101, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 47/2060 [00:03<02:40, 12.57it/s, loss=0.01, v_num=ih91]  cpu
cpu
Epoch 0:   2%|▏         | 48/2060 [00:03<02:40, 12.57it/s, loss=0.0101, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 49/2060 [00:03<02:39, 12.59it/s, loss=0.0102, v_num=ih91]cpu
cpu
Epoch 0:   2%|▏         | 50/2060 [00:03<02:40, 12.56it/s, loss=0.01, v_num=ih91]  cpu
cpu
Epoch 0:   2%|▏         | 51/2060 [00:04<02:40, 12.54it/s, loss=0.0101, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 52/2060 [00:04<02:40, 12.53it/s, loss=0.01, v_num=ih91]  cpu
cpu
Epoch 0:   3%|▎         | 53/2060 [00:04<02:40, 12.51it/s, loss=0.00963, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 54/2060 [00:04<02:40, 12.50it/s, loss=0.00983, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 55/2060 [00:04<02:40, 12.49it/s, loss=0.00988, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 56/2060 [00:04<02:40, 12.48it/s, loss=0.00982, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 57/2060 [00:04<02:40, 12.48it/s, loss=0.00995, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 58/2060 [00:04<02:40, 12.48it/s, loss=0.00981, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 59/2060 [00:04<02:40, 12.46it/s, loss=0.0101, v_num=ih91] cpu
cpu
Epoch 0:   3%|▎         | 60/2060 [00:04<02:40, 12.48it/s, loss=0.01, v_num=ih91]  cpu
cpu
Epoch 0:   3%|▎         | 61/2060 [00:04<02:40, 12.49it/s, loss=0.0099, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 62/2060 [00:04<02:39, 12.50it/s, loss=0.00991, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 63/2060 [00:05<02:39, 12.51it/s, loss=0.00989, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 64/2060 [00:05<02:39, 12.53it/s, loss=0.00944, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 65/2060 [00:05<02:38, 12.55it/s, loss=0.00962, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 66/2060 [00:05<02:38, 12.55it/s, loss=0.00958, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 67/2060 [00:05<02:38, 12.56it/s, loss=0.00979, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 68/2060 [00:05<02:38, 12.55it/s, loss=0.00972, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 69/2060 [00:05<02:38, 12.55it/s, loss=0.00935, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 70/2060 [00:05<02:39, 12.51it/s, loss=0.00939, v_num=ih91]cpu
cpu
Epoch 0:   3%|▎         | 71/2060 [00:05<02:38, 12.53it/s, loss=0.0093, v_num=ih91] cpu
cpu
Epoch 0:   3%|▎         | 72/2060 [00:05<02:38, 12.55it/s, loss=0.00892, v_num=ih91]cpu
cpu
Epoch 0:   4%|▎         | 73/2060 [00:05<02:38, 12.54it/s, loss=0.00902, v_num=ih91]cpu
cpu
Epoch 0:   4%|▎         | 74/2060 [00:05<02:39, 12.49it/s, loss=0.00871, v_num=ih91]cpu
cpu
Epoch 0:   4%|▎         | 75/2060 [00:06<02:38, 12.50it/s, loss=0.00813, v_num=ih91]cpu
cpu
Epoch 0:   4%|▎         | 76/2060 [00:06<02:38, 12.50it/s, loss=0.0084, v_num=ih91] cpu
cpu
Epoch 0:   4%|▎         | 77/2060 [00:06<02:38, 12.51it/s, loss=0.00832, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 78/2060 [00:06<02:38, 12.53it/s, loss=0.00853, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 79/2060 [00:06<02:37, 12.56it/s, loss=0.00861, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 80/2060 [00:06<02:37, 12.58it/s, loss=0.00865, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 81/2060 [00:06<02:37, 12.60it/s, loss=0.00922, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 82/2060 [00:06<02:36, 12.61it/s, loss=0.00932, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 83/2060 [00:06<02:36, 12.62it/s, loss=0.00925, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 84/2060 [00:06<02:36, 12.62it/s, loss=0.00966, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 85/2060 [00:06<02:36, 12.62it/s, loss=0.00983, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 86/2060 [00:06<02:36, 12.62it/s, loss=0.00982, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 87/2060 [00:06<02:36, 12.58it/s, loss=0.00961, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 88/2060 [00:07<02:36, 12.56it/s, loss=0.00956, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 89/2060 [00:07<02:36, 12.57it/s, loss=0.00957, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 90/2060 [00:07<02:36, 12.55it/s, loss=0.00977, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 91/2060 [00:07<02:39, 12.32it/s, loss=0.0101, v_num=ih91] cpu
cpu
Epoch 0:   4%|▍         | 92/2060 [00:07<02:39, 12.32it/s, loss=0.0103, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 93/2060 [00:07<02:39, 12.33it/s, loss=0.0108, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 94/2060 [00:07<02:39, 12.33it/s, loss=0.0109, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 95/2060 [00:07<02:39, 12.33it/s, loss=0.0112, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 96/2060 [00:07<02:39, 12.31it/s, loss=0.0106, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 97/2060 [00:07<02:39, 12.31it/s, loss=0.0107, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 98/2060 [00:07<02:39, 12.29it/s, loss=0.0105, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 99/2060 [00:08<02:40, 12.24it/s, loss=0.01, v_num=ih91]  cpu
cpu
Epoch 0:   5%|▍         | 100/2060 [00:08<02:40, 12.21it/s, loss=0.0101, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 101/2060 [00:08<02:40, 12.18it/s, loss=0.00981, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 102/2060 [00:08<02:41, 12.16it/s, loss=0.00977, v_num=ih91]cpu
cpu
Epoch 0:   5%|▌         | 103/2060 [00:08<02:41, 12.11it/s, loss=0.00947, v_num=ih91]cpu
cpu
Epoch 0:   4%|▍         | 82/2060 [00:06<02:36, 12.61it/s, loss=0.00932, v_num=ih91]cpu
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
cpuch 0:   4%|▍         | 82/2060 [00:06<02:36, 12.61it/s, loss=0.00932, v_num=ih91]cpu
Epoch 0:   5%|▌         | 109/2060 [00:09<02:43, 11.96it/s, loss=0.00873, v_num=ih91]cpu
cpu
Epoch 0:   5%|▌         | 110/2060 [00:09<02:43, 11.96it/s, loss=0.00827, v_num=ih91]cpu
cpu
Epoch 0:   5%|▌         | 111/2060 [00:09<02:43, 11.95it/s, loss=0.00801, v_num=ih91]cpu
cpu
Epoch 0:   5%|▌         | 112/2060 [00:09<02:42, 11.95it/s, loss=0.00807, v_num=ih91]cpu
cpu
Epoch 0:   5%|▌         | 113/2060 [00:09<02:42, 11.95it/s, loss=0.00776, v_num=ih91]cpu
cpu
cpuch 0:   4%|▍         | 82/2060 [00:06<02:36, 12.61it/s, loss=0.00932, v_num=ih91]cpu
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name    | Type    | Params
------------------------------------
0 | encoder | Encoder | 12.9 K
1 | decoder | Decoder | 12.9 K
------------------------------------
25.8 K    Trainable params
0         Non-trainable params
25.8 K    Total params
0.103     Total estimated model params size (MB)
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.
  rank_zero_warn(
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
Epoch 0:   1%|          | 19/2060 [00:01<02:42, 12.59it/s, loss=0.0097, v_num=ih91] cpu
cpu
Epoch 0:   1%|          | 19/2060 [00:01<02:42, 12.59it/s, loss=0.0097, v_num=ih91] cpu
Epoch 0:   3%|▎         | 72/2060 [00:05<02:27, 13.45it/s, loss=0.0101, v_num=ih91]cpuu
cpu
Epoch 0:   4%|▎         | 73/2060 [00:05<02:27, 13.45it/s, loss=0.0102, v_num=ih91]cpu
cpu
Epoch 0:   4%|▎         | 74/2060 [00:05<02:27, 13.47it/s, loss=0.0101, v_num=ih91]cpu
cpu
Epoch 0:   4%|▎         | 75/2060 [00:05<02:27, 13.49it/s, loss=0.00989, v_num=ih91]cpu
cpu
cpuch 0:   3%|▎         | 72/2060 [00:05<02:27, 13.45it/s, loss=0.0101, v_num=ih91]cpuu
Epoch 0:   5%|▍         | 99/2060 [00:07<02:22, 13.77it/s, loss=0.00875, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 100/2060 [00:07<02:22, 13.78it/s, loss=0.00876, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 101/2060 [00:07<02:21, 13.80it/s, loss=0.00896, v_num=ih91]cpu
cpu
Epoch 0:   5%|▍         | 102/2060 [00:07<02:21, 13.81it/s, loss=0.00892, v_num=ih91]cpu
cpu
Epoch 0:   5%|▌         | 103/2060 [00:07<02:21, 13.81it/s, loss=0.00882, v_num=ih91]cpu
cpu
Epoch 0:   5%|▌         | 104/2060 [00:07<02:21, 13.83it/s, loss=0.0091, v_num=ih91] cpu
cpu
Epoch 0:   5%|▌         | 105/2060 [00:07<02:21, 13.84it/s, loss=0.00948, v_num=ih91]cpu
cpu
Epoch 0:   6%|▌         | 125/2060 [00:09<02:20, 13.79it/s, loss=0.00944, v_num=ih91]cpu
cpu
Epoch 0:   6%|▌         | 126/2060 [00:09<02:20, 13.78it/s, loss=0.00982, v_num=ih91]cpu
cpu
Epoch 0:   6%|▌         | 127/2060 [00:09<02:20, 13.77it/s, loss=0.00976, v_num=ih91]cpu
cpu
Epoch 0:   6%|▌         | 128/2060 [00:09<02:20, 13.78it/s, loss=0.00952, v_num=ih91]cpu
cpu
Epoch 0:   6%|▋         | 129/2060 [00:09<02:20, 13.79it/s, loss=0.0094, v_num=ih91] cpu
cpu
Epoch 0:   6%|▋         | 130/2060 [00:09<02:19, 13.79it/s, loss=0.00957, v_num=ih91]cpu
cpu
Epoch 0:   6%|▋         | 131/2060 [00:09<02:19, 13.81it/s, loss=0.00951, v_num=ih91]cpu
cpu
Epoch 0:   6%|▋         | 132/2060 [00:09<02:19, 13.82it/s, loss=0.00988, v_num=ih91]cpu
cpu
Epoch 0:   6%|▋         | 133/2060 [00:09<02:19, 13.83it/s, loss=0.00946, v_num=ih91]cpu
cpu
cpuch 0:   6%|▌         | 125/2060 [00:09<02:20, 13.79it/s, loss=0.00944, v_num=ih91]cpu
Epoch 0:   7%|▋         | 152/2060 [00:11<02:18, 13.81it/s, loss=0.00958, v_num=ih91]cpu
cpu
Epoch 0:   7%|▋         | 153/2060 [00:11<02:18, 13.81it/s, loss=0.00955, v_num=ih91]cpu
cpu
Epoch 0:   7%|▋         | 154/2060 [00:11<02:17, 13.82it/s, loss=0.00934, v_num=ih91]cpu
cpu
Epoch 0:   8%|▊         | 155/2060 [00:11<02:17, 13.81it/s, loss=0.00949, v_num=ih91]cpu
cpu
Epoch 0:   8%|▊         | 156/2060 [00:11<02:17, 13.81it/s, loss=0.00885, v_num=ih91]cpu
cpu
Epoch 0:   8%|▊         | 157/2060 [00:11<02:17, 13.81it/s, loss=0.00869, v_num=ih91]cpu
cpu
Epoch 0:   8%|▊         | 158/2060 [00:11<02:18, 13.70it/s, loss=0.00819, v_num=ih91]cpu
cpu
Epoch 0:   8%|▊         | 159/2060 [00:11<02:18, 13.71it/s, loss=0.00852, v_num=ih91]cpu
cpu
Epoch 0:   8%|▊         | 160/2060 [00:11<02:18, 13.72it/s, loss=0.00829, v_num=ih91]
Epoch 0:   9%|▊         | 178/2060 [00:12<02:16, 13.75it/s, loss=0.00884, v_num=ih91]cpu
cpu
Epoch 0:   9%|▊         | 179/2060 [00:13<02:16, 13.76it/s, loss=0.00859, v_num=ih91]cpu
cpu
Epoch 0:   9%|▊         | 180/2060 [00:13<02:16, 13.76it/s, loss=0.00873, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 181/2060 [00:13<02:16, 13.77it/s, loss=0.00914, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 182/2060 [00:13<02:16, 13.78it/s, loss=0.00898, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 183/2060 [00:13<02:16, 13.78it/s, loss=0.00937, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 184/2060 [00:13<02:16, 13.79it/s, loss=0.00944, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 185/2060 [00:13<02:15, 13.79it/s, loss=0.00954, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 186/2060 [00:13<02:15, 13.80it/s, loss=0.00942, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 187/2060 [00:13<02:15, 13.80it/s, loss=0.00918, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 188/2060 [00:13<02:15, 13.81it/s, loss=0.00928, v_num=ih91]cpu
cpu
Epoch 0:   9%|▉         | 189/2060 [00:13<02:15, 13.82it/s, loss=0.00898, v_num=ih91]cpu
cpuch 0:   9%|▊         | 178/2060 [00:12<02:16, 13.75it/s, loss=0.00884, v_num=ih91]cpu
Epoch 0:  10%|▉         | 205/2060 [00:14<02:14, 13.83it/s, loss=0.00873, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 206/2060 [00:14<02:14, 13.83it/s, loss=0.00854, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 207/2060 [00:14<02:13, 13.83it/s, loss=0.00872, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 208/2060 [00:15<02:13, 13.83it/s, loss=0.00855, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 209/2060 [00:15<02:13, 13.83it/s, loss=0.00864, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 210/2060 [00:15<02:13, 13.84it/s, loss=0.00839, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 211/2060 [00:15<02:13, 13.83it/s, loss=0.00846, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 212/2060 [00:15<02:14, 13.75it/s, loss=0.0083, v_num=ih91] cpu
cpu
Epoch 0:  10%|█         | 213/2060 [00:15<02:14, 13.73it/s, loss=0.00818, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 214/2060 [00:15<02:14, 13.72it/s, loss=0.00787, v_num=ih91]cpu
cpu
Epoch 0:  10%|█         | 215/2060 [00:15<02:14, 13.71it/s, loss=0.00787, v_num=ih91]
Epoch 0:  11%|█         | 231/2060 [00:16<02:14, 13.62it/s, loss=0.00975, v_num=ih91]cpu
cpu
Epoch 0:  11%|█▏        | 232/2060 [00:17<02:14, 13.62it/s, loss=0.0099, v_num=ih91] cpu
cpu
Epoch 0:  11%|█▏        | 233/2060 [00:17<02:14, 13.62it/s, loss=0.00983, v_num=ih91]cpu
cpu
Epoch 0:  11%|█▏        | 234/2060 [00:17<02:14, 13.62it/s, loss=0.01, v_num=ih91]   cpu
cpu
Epoch 0:  11%|█▏        | 235/2060 [00:17<02:14, 13.62it/s, loss=0.0103, v_num=ih91]cpu
cpu
Epoch 0:  11%|█▏        | 236/2060 [00:17<02:14, 13.60it/s, loss=0.0102, v_num=ih91]cpu
cpu
Epoch 0:  12%|█▏        | 237/2060 [00:17<02:14, 13.59it/s, loss=0.0102, v_num=ih91]cpu
cpu
Epoch 0:  12%|█▏        | 238/2060 [00:17<02:14, 13.58it/s, loss=0.0103, v_num=ih91]cpu
cpu
Epoch 0:  12%|█▏        | 239/2060 [00:17<02:14, 13.57it/s, loss=0.0106, v_num=ih91]cpu
cpu
Epoch 0:  12%|█▏        | 240/2060 [00:17<02:14, 13.56it/s, loss=0.0104, v_num=ih91]cpu
cpu
cpuch 0:  11%|█         | 231/2060 [00:16<02:14, 13.62it/s, loss=0.00975, v_num=ih91]cpu
Epoch 0:  13%|█▎        | 258/2060 [00:19<02:14, 13.42it/s, loss=0.00851, v_num=ih91]cpu
cpu
cpuch 0:  11%|█         | 231/2060 [00:16<02:14, 13.62it/s, loss=0.00975, v_num=ih91]cpu
Epoch 0:  14%|█▍        | 284/2060 [00:21<02:13, 13.30it/s, loss=0.00805, v_num=ih91]cpu
cpuch 0:  14%|█▍        | 284/2060 [00:21<02:13, 13.30it/s, loss=0.00805, v_num=ih91]cpu
Epoch 0:  16%|█▋        | 337/2060 [00:26<02:14, 12.83it/s, loss=0.00896, v_num=ih91]cpu
cpuch 0:  16%|█▋        | 337/2060 [00:26<02:14, 12.83it/s, loss=0.00896, v_num=ih91]cpu
Epoch 0:  19%|█▉        | 390/2060 [00:31<02:13, 12.47it/s, loss=0.00738, v_num=ih91]cpu
Epoch 0:  19%|█▉        | 390/2060 [00:31<02:13, 12.47it/s, loss=0.00738, v_num=ih91]cpu
Epoch 0:  19%|█▉        | 390/2060 [00:31<02:13, 12.47it/s, loss=0.00738, v_num=ih91]cpu
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Epoch 0:  19%|█▉        | 390/2060 [00:31<02:13, 12.47it/s, loss=0.00738, v_num=ih91]cpu
/Users/nikolaushouben/opt/anaconda3/envs/LearningBerkeley/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
Epoch 0:  19%|█▉        | 390/2060 [00:31<02:13, 12.47it/s, loss=0.00738, v_num=ih91]cpu
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
